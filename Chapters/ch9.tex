\newpage
\section[Functions of Several Variables]{\hyperlink{toc}{Functions of Several Variables}}

\subsection{Banach Fixed Point Theorem}
\noindent Our goal in this chapter will be to work up to the Inverse Function Theorem. This chapter in Rudin begins by covering the necessary results in linear algebra; we will assume this has been covered in a prior course, so we will omit discussion of items 9.1-9.9. However, these can be read for a refresher of the material.

We will start off this chapter with discussion of the Banach fixed point theorem (also known as the Contraction principle), as it is independent of the rest of the chapter's content. It will be used in the proof of the inverse function theorem, but also applies in a more general setting.

\setcounter{rudin}{21}

\begin{definition}{Contractions}{9.22}
    Let $(X, d)$ be a metric space. Suppose there exists $c < 1$ such that the map $\phi: X \mapsto X$ satisfies $d(\phi(x), \phi(y)) \leq cd(x, y)$ for all $x, y \in X$ (that is, the images of $x, y$ are closer by a factor $c$ compared to the original $x, y$). Then, we call $\phi$ a \textbf{contraction} of $X$ into $X$.
\end{definition}
\begin{nlemma}{}{}
    Every contraction $\phi: X \mapsto X$ is uniformly continuous.
\end{nlemma}
\begin{nproof}
    Take $\delta = \frac{\e}{c}$ in the definition of uniform continuity. \qed
\end{nproof}

\begin{theorem}{Banach Fixed Point Theorem/Contraction Mapping Theorem}{9.23}
    Let $(X, d)$ be a complete metric space, and suppose $\phi: X \mapsto X$ is a contraction. Then, there exists a unique $x \in X$ such that $\phi(x) = x$. We call this x a \emph{fixed point}. 
\end{theorem}
\noindent The proof of the above theorem gives an algorithm to find $x$ which converges exponentially fast.

\begin{figure}[htbp]
    \centering
    \begin{tikzpicture}[scale=2]
        \draw[-latex, very thick] (0, 0) -- (2, 0);
        \draw[-latex, very thick] (0, 0) -- (0, 2);
        \draw[-latex, very thick] (0, 0) -- (-1, 0);
        \draw[blue] (-0.5, 0) .. controls (0, 0.6) and (0.5, 0.8 ).. (1.8, 1.4);
        \draw[fill = blue, draw = blue] (-0.5, 0) circle (1pt);
        \draw[fill = blue, draw = blue] (1.8, 1.4) circle (1pt);
        \draw[red] (-0.5, -0.5) -- (1.8, 1.8);
        \draw[fill = red, draw = red] (-0.5, -0.5) circle (1pt);
        \draw[fill = red, draw = red] (1.8, 1.8) circle (1pt);
        \draw[] (-0.4, 0) -- (-0.4, -0.15);
        \node[below] at (-0.4, -0.15) {$x_0$};
        \draw[dotted] (-0.4, 0) -- (-0.4, 0.12);
        \draw[dotted] (-0.4, 0.12) -- (0.13, 0.12);
        \draw[dotted] (0.11, 0) -- (0.11, 0.55);
        \draw[] (0.11, 0) -- (0.11, -0.15);
        \node[below] at (0.11, -0.15) {$x_1$};
        \draw[dotted] (0.11, 0.55) -- (0.55, 0.55);
        \draw[dotted] (0.55, 0) -- (0.55, 0.55);
        \draw[] (0.55, 0) -- (0.55, -0.15);
        \node[below] at (0.55, -0.15) {$x_2$};
    \end{tikzpicture}

    \caption{Visualization of the algorithm for finding the fixed point $x$ in Theorem \ref{thm:9.23} for the case where $X = \RR$. The fact that $\phi$ is a contraction makes it such that it has slope less than $1$. The fixed point is the point of intersection between $y = x$ and $y = \phi(x)$. The iterative algorithm sketched above gives an exponentially fast way of finding this point of intersection, by iteratively applying $\phi$ to the initial guess $x_0$.}
    \label{fig59}
\end{figure}

\begin{nproof}
    We first show uniqueness. If $\phi(x) = x$ and $\phi(y) = y$, then $d(\phi(x), \phi(y)) \leq cd(x, y)$. But since $c < 1$, $d(x, y) \leq cd(x, y)$ is only satisfied if $d(x, y) = 0$. Hence, $x = y$ and the fixed point is unique.

    We next show existence. Given $x_0 \in X$, let $x_1 = \phi(x_0)$, $x_2 = \phi(x_1) = \phi\circ \phi(x_0)$, $x_3 = \phi(x_2) = \phi\circ\phi\circ\phi(x_0)$ and so on, with $x_{n+1} = \phi(x_n) = \phi \circ \ldots \circ \phi(x_0)$ with the composition carried out $n+1$ times. The goal is to show this sequence is Cauchy, and has a limit which is a fixed point. We then have that $d(x_{n+1}, x_{n+2}) = d(\phi(x_n), \phi(x_{n+1})) \leq cd(x_{n+1}, x_n)$, so by induction, it follows that $d(x_{n+1}, x_n) \leq c^nd(x_1, x_0)$. Hence, for $n > m$ we have that:
    \begin{align*}
        d(x_n, x_m) &\leq \sum_{i=m+1}^n d(x_i, x_{i-1}) & \text{(Triangle Inequality)}
        \\ &\leq \sum_{i=m+1}^n c^{i-1}d(x_1, x_0)
        \\ &\leq \sum_{i=m+1}^\infty c^{i-1}d(x_1, x_0)
        \\ &= \frac{c^m}{1-c}d(x_1, x_0) & \text{(Convergent Geometric Series)}
    \end{align*}
    $c^m$ can be as small as we like by taking sufficiently large $m$, so $\set{x_n}$ is Cauchy, and by the completeness of $X$, $x_n \rightarrow x$ for some $x$. Since $\phi$ is a contraction, it is (uniformly) continuous by the above Lemma, and since $x_n \rightarrow x$, we have that $\phi(x_n) \rightarrow \phi(x)$, and hence:
    \begin{align*}
        \phi(x) = \linf \phi(x_n) = \linf x_{n+1} = x.
    \end{align*}
    This shows that $x$ is the desired fixed point. \qed
\end{nproof}

\subsection{Differentiation of Functions of Several Variables}
\noindent In this section we discuss the differentiation of functions $f: \RR^n \mapsto \RR^m$. It will be instructive to remind ourselves of the familiar case of $n = m = 1$. In this case, we defined the derivative as:
\begin{align*}
    f'(x) = \lim_{h \rightarrow 0}\frac{f(x + h) - f(x)}{h}
\end{align*}
where we would say $f$ was differentiable at $x$ if the above limit existed. We want to now try to generalize this notion to higher dimensions. It obviously does not apply directly, as in this general setting, $\v{f}(\v{x} + \v{h}) - \v{f}(\v{x})$ is a vector in $\RR^m$ and $\v{h}$ is a vector in $\RR^n$, and the division of such vectors is not well defined. Let us try to recast the $n = m = 1$ derivative into a form that lends itself better to generalization. We could equivalently write the above derivative expression as:
\begin{align*}
    \lim_{h \rightarrow 0}\frac{f(x + h) - f(x) - f'(x)h}{h} = 0
\end{align*}
and hence:
\begin{align*}
    \lim_{h \rightarrow 0}\abs{\frac{f(x + h) - f(x) - f'(x)h}{h}} = 0.
\end{align*}
Equivalently, we can say that $f(x + h) = f(x) + f'(x)h + r(h)$ with $\lim_{h \rightarrow \infty}\frac{r(h)}{h} = 0$; $r(h)$ is then the ``remainder term''. With this interpretation, $f'(x)h$ is the best linear approximation to $f(x + h) - f(x)$. This last characterization makes sense for general vectors in $\RR^k$, so let us define derivatives with this notion!

\setcounter{rudin}{10}

\begin{definition}{Derivatives of $f: \RR^n \mapsto \RR^n$}{9.11}
    Let $m, n \in \NN$ and $E \subset \RR^n$ be open. Define a function $\v{f}$ such that $\v{f}: E \mapsto \RR^m$, and let $\v{x} \in E$. We say that $f$ is \textbf{differentiable} at $\v{x}$ and has \textbf{derivative} $A \in L(\RR^n, \RR^m)$ ($\v{f}'(\v{x}) = A$) if:
    \begin{align*}
        \lim_{\v{h} \rightarrow \v{0}} \frac{\abs{\v{f}(\v{x} + \v{h}) - \v{f}(\v{x}) - A\v{h}}}{\abs{\v{h}}} = 0
    \end{align*}
\end{definition}
\noindent Note that the $\abs{}$ in the above definition refer to the $L_2$/Euclidean norm of $\RR^m$ (in the numerator) and $\RR^m$ (in the denominator) respectively. Furthermore, observe that $A$ is not just a number (as it is in the linear case) but an \emph{linear transformation} such that $A \in L(\RR^n, \RR^m)$, where $L(\RR^n, \RR^m)$ is the vector space of linear maps from $\RR^n$ to $\RR^m$. Equivalently, the above definition can be phrased as:
\begin{align*}
    \v{f}(\v{x} + \v{h}) = \v{f}(\v{x}) + A\v{h} + \v{r}(\v{h}) \text{ with } \lim_{\v{h} \rightarrow \v{0}} \frac{\abs{\v{r}(\v{h})}}{\abs{\v{h}}} = 0
\end{align*}
where again, $A\v{h}$ is the ``best linear approximation'' to $\v{f}(\v{x} + \v{h}) - \v{f}(\v{x})$. 

\begin{theorem}{Uniqueness of Higher Dimensional Derivatives}{9.12}
    The derivative $A$ defined above is unique.
\end{theorem}
\begin{nproof}
    Suppose $\v{f}(\v{x} + \v{h}) = \v{f}(\v{x}) + A_1\v{h} + \v{r}_1(\v{h})$ and $\v{f}(\v{x} + \v{h}) = \v{f}(\v{x}) + A_2\v{h} + \v{r}_2(\v{h})$ with $\v{r}_1(\v{h}) \in O(\v{h})$ and $\v{r}_2(\v{h}) \in O(\v{h})$. Let $B = A_1 - A_2$. Then, $0 = (A_1 - A_2)\v{h} + (\v{r}_1 - \v{r}_2)$. Therefore, $B\v{h} = \v{r}_2 - \v{r}_1$, so for $\v{h} \neq \v{0}$ and scalar $t \neq 0$ we have that:
    \begin{align*}
        \frac{\abs{B\v{h}}}{\abs{\v{h}}} = \frac{\abs{B(t\v{h})}}{\abs{t\v{h}}} \leq \frac{\abs{\v{r}_1(t\v{h})}}{\abs{t\v{h}}} + \frac{\abs{\v{r}_2(t\v{h})}}{\abs{t\v{h}}} \quad \text{(Triangle Inequality)}
    \end{align*} 
    Letting $t$ go to zero, we have that the RHS goes to zero. Hence, $\frac{\abs{B\v{h}}}{\abs{\v{h}}} \rightarrow 0$. But $\frac{\abs{B\v{h}}}{\abs{\v{h}}}$ is independent of $t$, so it must be zero. Hence, $B$ is the zero map and hence $A_1 = A_2$. We conclude that the derivative $A$ is unique. \qed
\end{nproof}



\subsection{The Inverse Function Theorem}

