\section[Some Special Functions]{\hyperlink{toc}{Some Special Functions}}

\subsection{Power Series, Revisited}
Recall our definition of power series (Definition \ref{def:3.38}), functions of the form:
\begin{align*}
    f(x) = \sum_{n=0}^\infty c_n x^n.
\end{align*}
Also recall the radius of convergence (Theorem \ref{thm:3.39}) of such power series, defined as:
\begin{align*}
    R = \frac{1}{\limsup_{n \rightarrow \infty}\sqrt[n]{\abs{c_n}}}.
\end{align*}
Note that if $\limsup_{n \rightarrow \infty}\sqrt[n]{\abs{c_n}} = \infty$, then $R = 0$, and if $\limsup_{n \rightarrow \infty}\sqrt[n]{\abs{c_n}} = 0$, then $R = \infty$. The series converges absolutely for $\abs{x} < R$ and diverges for $\abs{x} > R$.

\begin{figure}[htbp]
    \centering
    \begin{tikzpicture}[scale=2]
        \draw[latex-latex, very thick] (-2, 0) -- (2, 0);
        \draw[] (0, 0) -- (0, -0.15);
        \draw[] (1, 0) -- (1, -0.15);
        \draw[] (-1, 0) -- (-1, -0.15);
        \node[below] at (0, -0.15) {$0$};
        \node[below] at (1, -0.15) {$R$};
        \node[below] at (-1, -0.15) {$-R$};
        \draw[<->] (1, 0.15) -- (-1, 0.15);
        \draw[<->] (1, 0.15) -- (2, 0.15);
        \draw[<->] (-1, 0.15) -- (-2, 0.15);
        \node[above] at (0, 0.15) {absolute convergence};
        \node[above] at (1.5, 0.15) {divergence};
        \node[above] at (-1.5, 0.15) {divergence};
    \end{tikzpicture}
    
    \caption{Visualization of the radius of convergence for $f(x) = \sum_{n=0}^\infty c_nx^n$, $x \in \RR$.}
    \label{fig49}
\end{figure}

\begin{theorem}{}{8.1}
    If $\sum_{n=0}^\infty c_nx^n$ converges for $\abs{x} < R$, let $f(x) = \sum_{n=0}^\infty c_n x^n$ for $\abs{x} < R$. Then, the series converges uniformly on $[-R + \e, R - \e]$ for any $\e > 0$, $f$ is differentiable (and hence continuous) on $(-R, R)$ and $f'(x) = \sum_{n=0}^\infty nc_nx^{n-1}$. 
\end{theorem}

\begin{nproof}
    We first show the uniform convergence on $[-R + \e, R - \e]$. For $\abs{x} \leq R - \e$, we have that $\abs{c_nx^n} \leq \abs{x_n}(R - \e)^n$. Since $\sum_n \abs{c_n}(R - \e)^n < \infty$ (by the assumed absolute convergence on $(-R, R)$), we have that $\sum_n c_nx^n$ converges uniformly in $\abs{x} \leq R - \e$ by the M-test (Theorem \ref{thm:7.10}).

    We next prove the claim about the differentiability/derivative of $f$. The radius of convergence of $\sum_n nc_n x^{n-1}$ is:
    \begin{align*}
        \frac{1}{\limsup_{n \rightarrow \infty}\sqrt[n]{\abs{nc_n}}} = \frac{1}{\limsup_{n \rightarrow \infty}\sqrt[n]{\abs{c_n}}} = R
    \end{align*}
    so since $f$ converges in $(-R, R)$, so does $\sum_n nc_nx^{n-1}$. Now, let $s_n(x) = \sum_{m=0}^n c_mx^m$. Then, by the linearity of the derivative we have that $s_n'(x) = \sum_{m=1}^n mc_mx^{m-1}$. By the first part of the proof, we have that $s_n'(x) \rightarrow \sum_{m=1}^\infty mc_mx^{m-1}$ uniformly on $[-R + \e, R - \e]$. Since $s_n(x) \rightarrow f(x)$ uniformly, by Theorem \ref{thm:7.17}, we have that $f'$ exists on $[-R +\e, R - \e]$ and $f'(x) = \sum_{m=1}^\infty mc_mx^{m-1}$. Since $\e$ is arbitrary, $f'$ exists and is equal to $\sum_{m=1}^\infty mc_mx^{m-1}$ for all $x \in (-R, R)$. \qed 
\end{nproof}
\noindent As a remark, note that we can (interestingly) prove the differentiability of $f$ on $(-R, R)$ from the uniform convergence on $[-R + \e, R - \e]$.

\begin{ncorollary}{}{}
    If $f(x) = \sum_{n=0}^\infty c_nx^n$ converges for $\abs{x} < R$, then $f^{(k)}(x)$ exists for all $k \in \NN$ and for all $x \in (-R, R)$. It is given by:
    \begin{align*}
        f^{(k)}(x) = \sum_{n=k}^\infty n(n-1)\ldots(n-k+1)c_nx^{n-k} \quad (*)
    \end{align*}
    and consequently, we have that $c_k = \frac{f^{(k)(0)}}{k!}$, so $f(x) = \sum_{n=0}^\infty \frac{f^{(n)}(0)}{n!}x^n$.
\end{ncorollary}
\noindent Compare the above Corollary to Taylor's theorem (Theorem \ref{thm:5.15}). Here, we take our taylor polynomial and extend it to an infinite series (the limit of polynomials). 

\begin{nproof}
    By Theorem \ref{thm:8.1}, we have that $f'(x) = \sum_{n=1}^\infty nc_nx^{n-1}$ and $f''(x) = \sum_{n=2}^\infty n(n-1)c_nx^{n-2}$ and so on. Setting $x = 0$ in $(*)$, we have that $f^{(k)}(0) = n(n-1)\ldots 1 c_n = n! c_k$, so $c_n = \frac{f^{(k)}(0)}{n!}$. \qed
\end{nproof}
\noindent Recall the definition of \emph{analytic functions}, which are infinite differentiable and can be represented as sums or series of derivatives evaluated at zero. 

\begin{nexample}{}{}
    As we discussed in Chapter 5, there are infinitely differentiable functions that are not analytic. Let:
    \begin{align*}
        f(x) = \begin{cases}
            \exp(-\frac{1}{x^2}) & x \neq 0
            \\ 0 & x = 0
        \end{cases}
    \end{align*}
    By Theorem \ref{thm:8.1}, $f$ is infinitely differentiable, and $f^{(n)}(0) = 0$ for all $n \in \NN$. But, $f(x) \neq 0$ except at $x = 0$. Hence, $f(x) \neq \sum_{n=0}^\infty \frac{f^{(n)}(0)}{n!}x^n$ except at $x = 0$. This is true despite the fact that the RHS converges to zero for all $x \in \RR$. 
\end{nexample}

\begin{nexample}{}{}
    Bump functions are continuous, infinitely differentiable functions of compact support (it is zero outside of a compact set). For example, 
    \begin{align*}
        f(x) = \begin{cases}
            \exp(-\frac{1}{1-x^2}) & x \in (-1, 1)
            \\ 0 & \abs{x} \geq 1
        \end{cases}
    \end{align*}
    is an example of a bump function. Such functions are very useful in the study of functional analysis and PDEs.
\end{nexample}
\begin{figure}[htbp]
    \centering
    \begin{tikzpicture}[scale=1.5]
        \draw[-latex, very thick] (0, 0) -- (0, 2);
        \draw[latex-latex, very thick] (-2, 0) -- (2, 0);
        \draw[blue, thick, smooth, samples = 100, domain=-0.999:0.999, variable = \x] plot(\x, {3*exp(-1/(1-(\x)*(\x)))});
        \draw[->, blue, thick] (1, 0) -- (1.95, 0);
        \draw[->, blue, thick] (-1, 0) -- (-1.95, 0);
        \draw[] (1, 0) -- (1, -0.15);
        \draw[] (-1, 0) -- (-1, -0.15);
        \node[below] at (1, -0.15) {$1$};
        \node[below] at (-1, -0.15) {$-1$};
        \node[right] at (0, 1.23) {$e^{-1}$};
    \end{tikzpicture}
    
    \caption{Plot of the bump function $f$ from the above example.}
    \label{fig50}
\end{figure}

\noindent Before we continue, some remarks on the radius of convergence are in order. Of course, the definition of $R = \frac{1}{\limsup_{n \rightarrow \infty}\sqrt[n]{\abs{c_n}}}$ always holds. But in practice, the ratio test is much nicer to use (though it does not always work). We can make the observation that:
\begin{align*}
    \abs{\frac{c_{n+1}x^{n+1}}{c_nx^n}} = \abs{x}\abs{\frac{c_{n+1}}{c_n}} \rightarrow \abs{x}L
\end{align*}
Where we take the $n \rightarrow \infty$ limit in the final expression (assuming the limit exists). We have that the power series converges if $\abs{x} < \frac{1}{L}$ and diverges if $\abs{x} > \frac{1}{L}$. So, when the limit exists, we can write:
\begin{align*}
    R = \frac{1}{\linf\abs{\frac{c_{n+1}}{c_n}}}.
\end{align*}
In general, by Theorem 3.37 (not covered in lecture in 320, see Rudin), we have that:
\begin{align*}
    \frac{1}{\limsup_{n \rightarrow \infty}\abs{\frac{c_{n+1}}{c_n}}} \leq R \leq \frac{1}{\liminf_{n \rightarrow \infty}\abs{\frac{c_{n+1}}{c_n}}}
\end{align*}

\begin{theorem}{Abel's Theorem}{8.2}
    Suppose $\sum_{n=0}^\infty c_n$ converges (perhaps conditionally). Let $f(x) = \sum_{n=0}^\infty c_nx^n$. Then, $f(x)$ converges if $\abs{x} < 1$, and $\lim_{x \rightarrow 1^{-}}f(x) = \sum_{n=0}^\infty c_n$. 
\end{theorem}
\begin{nproof}
    For the first claim, we have that $\limsup_{n \rightarrow \infty}\sqrt[n]{\abs{c_n}} \leq 1$ so by the root test, $\sum_n c_n x^n$ has $R \geq 1$. The interesting case is when $R = 1$ (as if $R > 1$, then $f$ is continuous on $(-R, R)$ and the result follows immediately). In this case, $f(x)$ converges if $\abs{x} < 1$. Let $s_n = \sum_{m=0}^n c_m$ and $s = \sum_{m=0}^\infty c_m = \lim_{n \rightarrow \infty}s_n$. Let $s_{-1} = 0$. Then, we have that $s_n - s_{n-1} = c_n$ for $n \geq 0$.

    Let $\e > 0$. We wish to show that there exists $\delta > 0$ such that for $1 -\delta < x < 1$ we have that $\abs{f(x) - s} < \e$. We start with the partial sum of $f(x)$. For $\abs{x} < 1$, we have:
    \begin{align*}
        \sum_{m=0}^n c_mx^m &= \sum_{m=0}^n (s_m - s_{m-1})x^m
        \\ &= \sum_{m=0}^n s_mx^m - x\sum_{m=0}^{n-1}s_m x^m
        \\ &= (1-x)\sum_{m=0}^n s_mx^m + s_nx^{n+1}.
    \end{align*}
    Now, let $n \rightarrow \infty$. We then have that $s_nx^{n+1} \rightarrow 0$ as $s_n \rightarrow s$ and $x^{n+1} \rightarrow 0$. We then have that $f(x) = (1-x)\sum_{m=0}^n s_mx^m + 0$, and using that $\sum_{m=0}^\infty x^m = \frac{1}{1-x}$, we obtain that:
    \begin{align*}
        \abs{f(x) - s} = \abs{(1-x)\sum_{m=0}^\infty(s_m - s)x^m} \leq \abs{1 - x}\sum_{m=0}^\infty \abs{s_m - s}\abs{x}^m.
    \end{align*}
    Now, choose $N$ such that $m \geq N$ implies $\abs{s_m - s} < \frac{\e}{2}$. Let $x \in (0, 1)$, and then:
    \begin{align*}
        \abs{f(x) - s} \leq (1-x)\sum_{m=0}^N\abs{s_m - s}x^n + (1-x)\frac{\e}{2}\frac{1}{1-x}
    \end{align*}
    The second term on the RHS is bounded using the geometric series. The first term is a polyonimal in $x$ and hence continuous everywhere (including at $x = 1$). It equals $0$ at $x = 1$, so it has absolute value less than $\frac{\e}{2}$ if $x \in (1-\delta, 1)$ for some $\delta$. Hence:
    \begin{align*}
        \abs{f(x) - s} < \frac{\e}{2} + \frac{\e}{2} = \e
    \end{align*}
    \noindent See Rudin page 175 for an application of this Theorem to prove Theorem 3.51 in a different way. Not that for the case where $\sum_n c_n = \infty$, the theorem still holds, with $\lim_{x \rightarrow 1^-}\sum_{n=0}^\infty c_nx^n = \infty$. \qed
\end{nproof}

\begin{theorem}{}{8.3}
    Suppose $\sum_{i=1}^\infty \left(\sum_{j=1}^\infty \abs{a_{ij}}\right) < \infty$. Then, $\sum_{i=0}^\infty \sum_{j=0}^\infty a_{ij} = \sum_{j=0}^\infty \sum_{i=0}^\infty a_{ij}$ (and both converge).
\end{theorem}
\begin{nproof}
    Rudin uses an overly clever proof. See HW7Q2 for a more natural one. \qed
\end{nproof}

\begin{theorem}{}{8.4}
    Suppose $f(x) = \sum_{n=0}^\infty c_nx^n$ has radius of convergence $R$ (Taylor series of $f$ at 0/Maclaurin series). Let $\abs{a} < R$. Then, $f(x) = \sum_{n=0}^\infty \frac{f^{(n)}(0)}{n!}(x-a)^n$ for (at least) $\abs{x - a} < R - \abs{a}$. 
\end{theorem}
\begin{figure}[htbp]
    \centering
    \begin{tikzpicture}[scale=2]
        \draw[latex-latex, very thick] (-2, 0) -- (2, 0);
        \draw[] (0, 0) -- (0, -0.15);
        \draw[] (1, 0) -- (1, -0.15);
        \draw[] (-1, 0) -- (-1, -0.15);
        \draw[] (0.5, 0) -- (0.5, -0.15);
        \node[below] at (0, -0.15) {$0$};
        \node[below] at (1, -0.15) {$R$};
        \node[below] at (-1, -0.15) {$-R$};
        \node[below] at (0.5, -0.15) {$a$};
        \draw[<->, red] (1, 0.15) -- (0, 0.15);
    \end{tikzpicture}
    
    \caption{Visualization of Theorem \ref{thm:8.4}. The new Taylor series around $x = a$ converges in the region up to the radius of convergence of the original series.}
    \label{fig51}
\end{figure}

\begin{nproof}
    We have that $f(x) = \sum_{n=0}^\infty c_n\left[(x - a) + a\right]^n = \sum_{n=0}^\infty c_n\sum_{m=0}^n \binom{n}{m}(x-a)^ma^{n-m}$, and we want to find a way to interchange the order of summation. By Theorem \ref{thm:8.3}, the interchange is permitted if:
    \begin{align*}
        \sum_{n=0}^\infty\sum_{m=0}^n \abs{c_n}\binom{n}{m}\abs{x-a}^m\abs{a}^{n-m} < \infty
    \end{align*}
    but the above is equaivalent to:
    \begin{align*}
        \sum_{n=0}^\infty \abs{c_n}\left(\abs{x - a} + \abs{a}\right)^n
    \end{align*}
    which converges if $\abs{x - a} + \abs{a} < R$. Therefore:
    \begin{align*}
        f(x) = \sum_{m=0}^\infty \sum_{n=m}^\infty c_n\binom{n}{m}a^{n-m}(x - a)^m
    \end{align*}
    over $n \geq m$. We need to show that these new coefficients $\left(\sum_{n=m}^\infty c_n\binom{n}{m}a^{n-m}\right)$ are equal to $\frac{f^{(m)}(a)}{m!}$. Expanding out, we have that:
    \begin{align*}
        \sum_{n=m}^\infty c_n\binom{n}{m}a^{n-m} = \frac{1}{m!}n(n-1)\ldots (n-m+1)c_na^{n-m} = \frac{1}{m!}f^{(m)}(a)
    \end{align*}
    where in the last equality we use Theorem \ref{thm:8.1}. We conclude that:
    \begin{align*}
        f(x) = \sum_{m=0}\frac{f^{(m)}(a)}{m!}(x-a)^m \text{ for } \abs{x - a} < R - \abs{a}
    \end{align*}
\end{nproof}

\begin{nexample}{}{}
    Let $f(x) = \sum_{n=0}^\infty x^n$. If $\abs{x} < 1$, then the series converges and $f(x) = \frac{1}{1-x}$ ($R = 1$). Choose $a = -\frac{1}{2}$ (we look at the Taylor serires around $x = -\frac{1}{2}$). For any $\abs{x} < 1$, we have that $f^{(n)}(x) = \frac{n!}{(1-x)^{n+1}}$, so therefore:
    \begin{align*}
        f^{(n)}(-\frac{1}{2}) = \frac{n!}{(1+\frac{1}{2})^{n+1}} = \left(\frac{2}{3}\right)^{n+1}n!.
    \end{align*}
    By Theorem \ref{thm:8.4}, we then have that:
    \begin{align*}
        f(x) = \sum_{n=0}^\infty \frac{\left(\frac{2}{3}\right)^{n+1}n!}{n!}(x - a)^n = \sum_{n=0}^\infty \left(\frac{2}{3}\right)^{n+1}(x + \frac{1}{2})^n
    \end{align*}
    which is valid for $\abs{x - (-\frac{1}{2})} < 1 - \abs{-\frac{1}{2}}$ and hence if $\abs{x + \frac{1}{2}} < \frac{1}{2}$. Note that in fact, $\sum_{n=0}^\infty\left(\frac{2}{3}\right)^{n+1}(x - a)^n$ converges whenever $\abs{\frac{2}{3}(x + \frac{1}{2})} < 1$, in other words, whenever $\abs{x + \frac{1}{2}} < \frac{3}{2}$, so the series converges in $(-\frac{3}{2}, \frac{1}{2})$. 
\end{nexample}
\begin{figure}[htbp]
    \centering
    \begin{tikzpicture}[scale=1.5]
        \draw[-latex, very thick] (0, 0) -- (0, 2);
        \draw[latex-latex, very thick] (-2, 0) -- (2, 0);
        \draw[blue, thick, smooth, <->, samples = 100, domain=-1.8:0.5, variable = \x] plot(\x, {1/(1 - (\x))});
        \draw[] (1, 0) -- (1, -0.15);
        \draw[] (-1, 0) -- (-1, -0.15);
        \draw[] (-0.5, 0) -- (-0.5, -0.15);
        \draw[] (-1.5, 0) -- (-1.5, -0.15);
        \draw[] (0.5, 0) -- (0.5, -0.15);
        \node[below] at (0.5, -0.15) {$\frac{1}{2}$};
        \node[below] at (1, -0.15) {$1$};
        \node[below] at (-1, -0.15) {$-1$};
        \node[below] at (-0.5, -0.15) {$-\frac{1}{2}$};
        \node[below] at (-1.5, -0.15) {$-\frac{3}{2}$};
        \node[above, text = blue] at (-1.5, 0.5) {$y = \frac{1}{1-x}$};
        \draw[dotted] (1, 2) -- (1, -0.15);
        \draw[<->, red] (-1.5, 0.15) -- (0.5, 0.15);
    \end{tikzpicture}
    
    \caption{Plot of $y = \frac{1}{1-x}$ and region for which the Taylor series around $a = -\frac{1}{2}$ is valid (red).}
    \label{fig52}
\end{figure}
\noindent As a remark, note that although Theorem \ref{thm:8.4} only guarantees convergence up to the previous boundary, in general the Taylor series will converge up to the nearest singularity. The above theorem is a good example of \emph{analytic continuation}, where a representation of a function converges in a larger interval than the original series (notice that the above taylor series for $f$ around $a = -\frac{1}{2}$ converges up to $-\frac{3}{2}$, when the original series only converged up to $-1$). We extend the function to a larger interval.

Note that there is another way to obtain the Taylor series around $a = -\frac{1}{2}$ (in a technique reminiscent of that used in MATH 300). We can cleverly manipulate the original expression and the geometric series formula to observe that:
\begin{align*}
    f(x) = \frac{1}{1 - x} = \frac{1}{1 - x + \frac{1}{2} - \frac{1}{2}} = \frac{2}{3}\frac{1}{1 - \frac{2}{3}(x + \frac{1}{2})} = \frac{2}{3}\sum_{n=0}^\infty \left(\frac{2}{3}\right)^n (x + \frac{1}{2})^n.
\end{align*}

\begin{theorem}{Principle of Permanence of Form}{8.5}
    Suppose $\sum_n a_n x^n$ and $\sum_n b_n x^n$ each have radius of convergence larger or equal to $R$. Suppose $D \subset (-R, R)$ has a limit point in $(-R, R)$ (for example, $D = \set{\frac{R}{n}: n = 2, 3, \ldots}$ with limit point $0$). If $\sum_n a_nx^n = \sum_n b_n x^n$ in all $x \in D$, then $a_n = b_n$ for all $n \in \NN$ and $\sum_n a_nx^n$ and $\sum_n b_n x^n$ for all $x \in (-R, R)$. 
\end{theorem}
\noindent Note that this also holds for complex variables, so it can be a way of taking things we know in a real context and promoting it to a complex context. We will first prove a lemma.

\begin{nlemma}{ (Problem 2.6)}{}
    Let $E \subset X$ for a metric space $X$. Then the set $E'$ of limit points of $E$ is closed.
\end{nlemma}
\begin{nproof}
    Let $x$ be a limit point of $E'$. We wish to show that $x \in E'$. Let $\delta > 0$. Since $x$ is a limit point of $E'$, for any $r > 0$ there exists some $y \in E'$, $y \neq x$ such that $y \in N_r(x)$. Since $y \in E'$, for any $\delta - r > 0$ we have that $N_{\delta-r}(y)$ contains a point $z$ of $E$. Therefore, we have that:
    \begin{align*}
        d(x, z) \leq d(x, y) + d(y, z) < r + \delta - r = \delta
    \end{align*}
    so any neighbourhood $N_\delta(x)$ of $x$ contains some point of $E$. Hence, $x$ is a limit point of $E$ and $x \in E'$. Hence $E'$ is closed. \qed
\end{nproof}
\noindent We now move to the proof of the theorem.
\begin{nproof}
    Let $c_n = a_n - b_n$ and $f(x) = \sum_n c_nx^n$. Then, $f(x) = 0$ for all $x \in D$. Let $E = \set{x \in (-R, R): f(x) = 0}$, so $D \subset E$. We want to show that $E = (-R, R)$. Let $A = E' \cap (-R, R)$. By hypothesis, $A \neq \emptyset$ as $D$ has a limit point in $(-R, R)$. Let $B = (-R, R) \setminus A$. Then, $(-R, R) = A \cup B$ and $A \cap B = \emptyset$. By the above Lemma, the set of limit points $E'$ is closed. Hence, $A$ is closed and $B$ is open. We claim that $A$ is also open.

    To see that this is the case, we show that $E'$ is open (then $E' \cap (-R, R)$ is open as a finite intersection of open sets is open). Let $x_0 \in A$. Then, there exists $d_n$ such that $f(x) = \sum_n d_n(x - x_0)^n$ for $\abs{x - x_0} < R - \abs{x_0}$ by Theorem \ref{thm:8.4}. We will show that $d_n = 0$ for all $n$, showing that $f(x) = 0$ for all $x \in I_0 = N_{R - \abs{x_0}}(x_0)$, and therefore that $A$ is open. Suppose for the sake of contradiction that this is false. Then, there exists $k$ such that $d_k \neq 0$, i.e. $f(x) = \sum_{n=k}^\infty d_n(x - x_0)^n$ with $d_k \neq 0$. So, $f(x) = (x-x_0)^k\sum_{n=0}^\infty d_{n+k}(x - x_0)^n = g(x)$. Then, $g(x_0) = d_k \neq 0$. By the continuity of $g$, there exists $\delta > 0$ such that $g(x) \neq 0$ for $\abs{x - x_0} < \delta$. But then, $f(x) = (x - x_0)^kg(x) \neq 0$ if $0 < \abs{x - x_0} < \delta$. But then we have that there exists a neighbourhood of $x_0$ such that $f(x) \neq 0$, but then $x_0$ cannot be a limit point of $\set{x: f(x) = 0}$. So, $x_0 \notin A$, which is a contradiction. Hence $d_n = 0$ for all $n \in \NN$, and $A$ is open. 
    
    Given the claim, we have that $A \cap \overline{B} = \overline{A} \cap B = \emptyset$ and hence $A$ and $B$ are separated sets. Since $(-R, R)$ is connected and equals $A \cup B$, one of $A, B$ must be empty. It cannot be $A$ (as $E' \neq \emptyset$ by assumption) so it must be $B$. Hence $B = \emptyset$ and $A = (-R, R) = E'$. This means that any $x \in (-R, R)$ is a limit point of $E$, so there exists $\set{x_n} \subset E$ such that $x_n \rightarrow x$. Since $f$ is continuous on $(-R, R)$, we have that $f(x) = \linf f(x_n) = 0$. So, $x \in E$ and $E = (-R, R)$. \qed
\end{nproof}
\noindent Having proven some more properties of power series, we now move to formal definitions of the exponential/logarithmic/trigonometric functions and a from-first-principles proof of their properties. 

\subsection{: The Exponential Function}
Recall Definition \ref{def:3.30}, where we defined $e = \sum_{n=0}^\infty \frac{1}{n!}$. We also showed in Theorem \ref{thm:3.31} that $e = \linf\left(1 + \frac{1}{n}\right)^n$. We also know that $2 < e < 3$, and  by Theorem \ref{thm:3.32} that $e$ is irrational. We now define a power series based on $e$. 

\begin{ndef}{: The Exponential Function}{}
    For $z \in \CC$, we define:
    \begin{align*}
        E(z) = \sum_{n=0}^\infty \frac{z^n}{n!}
    \end{align*}
\end{ndef}
\noindent In particular, we note that $E(1) = e$. Also note that (from Example \ref{exam:3.40}) that the above power series has radius of convergence $R = \infty$. We will show with the next sequence of theorems that $E(z)$ coincides with the exponential function $\exp(z) = e^z$ that we are familiar with.

\begin{ntheorem}{: Addition Formula}{}
    For $z, w \in \CC$, we have that $E(z + w) = E(z)E(w)$.
\end{ntheorem}
\begin{nproof}
    From the defintion, we have that:
    \begin{align*}
        E(z)E(w) &= \sum_{n=0}^\infty \frac{z^n}{n!}\sum_{m=0}^\infty\frac{w^m}{m!}
        \\ &= \sum_{n=0}^\infty \sum_{k=0}^n \frac{z^k}{k!}\frac{w^{n-k}}{(n-k)!}
        \\ &= \sum_{n=0}^\infty \frac{1}{n!}\sum_{k=0}^n\binom{n}{k}z^kw^{n-k}
        \\ &= \sum_{n=0}^\infty \frac{1}{n!}(z + w)^n
        \\ &= E(z + w)
    \end{align*}
    where in the second equality we use Theorem \ref{thm:3.50} for the multiplication of series. \qed
\end{nproof}
\noindent We can now use this addition formula to show how $E(p)$ coincides with $\exp(p)$:

\begin{ntheorem}{}{}
    For $p \in \QQ, E(p) = e^p$. 
\end{ntheorem}
\begin{nproof}
    Setting $z = w = 1$ in the addition formula above, we note that:
    \begin{align*}
        E(2) = E(1 + 1) = E(1)^2 = e^2, \quad E(3) = E(2 + 1) = E(2)E(1) = e^2e^1 = e^3
    \end{align*}
    so by induction, it follows that for $n \in \NN$:
    \begin{align*}
        E(n) = e^n
    \end{align*}
    Furthermore, we observe that:
    \begin{align*}
        E(z)E(-z) = E(z - z) = E(0) = 1
    \end{align*}
    so it follows that $E(z) = \frac{1}{E(z)}$. Hence, $E(-N) = \frac{1}{e^N} = e^{-N}$ for $N \in \NN$. Finally, let $p = \frac{m}{n}$ with $m, n \in \NN$. Then, we have that:
    \begin{align*}
        e^m = E(m) = E(np) = E(p + \ldots + p) = E(p)^n
    \end{align*}
    so therefore:
    \begin{align*}
        E(\frac{m}{n}) = E(p) = e^{\frac{m}{n}}
    \end{align*}
    Additionally, we have that:
    \begin{align*}
        1 = E(p - p) = E(p)E(-p) = e^pE(-p) \implies E(-p) = \frac{1}{e^p}.
    \end{align*}
    We have hence successfully shown that $E(p) = e^p$ for $p \in \QQ$. \qed
\end{nproof}
\noindent Next, we will extend the theorem for real exponents and study properties. We will have to come up with a different definition for an exponential of irrational powers; note that so far, our definition for exponentials $a^x$ only could accomodate rational $x$. 

\begin{ndef}{: Real Exponentials}{}
    For $x \in \RR$, let:
    \begin{align*}
        e^x = E(x) = \sum_{n=0}^\infty \frac{x^n}{n!}
    \end{align*}
\end{ndef}
\noindent The above defintion defines $x \mapsto e^x$ as an analytic function on $\RR$, which is therefore infinitely differentiable on $\RR$. Futhermore, the derivatives have a certain property.
\begin{ntheorem}{}{}
    $\od{}{x}e^x = e^x$.
\end{ntheorem}
\begin{nproof}
    By the defintion of $e^x$ we have that:
    \begin{align*}
        \dod{}{x}e^x = \sum_{n=0}^\infty \dod{}{x}\left(\frac{x^n}{n!}\right) = \sum_{n=0}^\infty \left(\frac{nx^{n-1}}{(n-1)!}\right) = \sum_{n=1}^\infty \frac{x^{n-1}}{(n-1)!} = \sum_{n=0}^\infty \frac{x^n}{n!} = e^x
    \end{align*} \qed
\end{nproof}

\begin{ntheorem}{}{}
    $e^x > 0$ for all $x \in \RR$.
\end{ntheorem}
\begin{nproof}
    Since $e^{x+y} = e^xe^y$ by the addition formula, we have for any $x \in \RR$ that $e^{x}e^{-x} = 1$. Since $e^x > 0$ if $x \geq 0$ by definition, it follows that $e^{-x} > 0$ as well if $e^{x}e^{-x} = 1 > 0$ is to be satisfied. \qed
\end{nproof}

\begin{ntheorem}{}{}
    $e^x$ is strictly increasing and strictly convex.
\end{ntheorem}
\begin{nproof}
    Since $\od{}{x} e^x = e^x > 0$ and $\od[2]{}{x} e^x = e^x > 0$ by the two previous theorems, we conclude that $e^x$ is strictly increasing and convex by Theorems \ref{thm:5.11} (and its extension). \qed
\end{nproof}

\begin{ntheorem}{: Superpolynomial Asymptotic Growth}{}
    $\lim_{x \rightarrow \infty} \frac{e^x}{x^n} = \infty$.
\end{ntheorem}
\begin{nproof}
    For $n \geq 0$, the defintion shows that:
    \begin{align*}
        e^x > \frac{x^{n+1}}{(n+1)!}
    \end{align*}
    if $x > 0$, as on the RHS we drop all but the $k = n+1$ term in the series (and all of the terms are positive for $x > 0$). Rearranging, we have that:
    \begin{align*}
        \frac{e^x}{x^n} > \frac{x}{(n+1)!}
    \end{align*}
    and the claim follows by taking $x \rightarrow \infty$. \qed
\end{nproof}
\noindent The above theorem shows that $e^x$ grows faster than any power of $x$. We also obtain the Corollary:
\begin{ncorollary}{}{}
    $\lim_{x \rightarrow \infty} e^x = \infty$, $\lim_{x \rightarrow \infty} e^{-x} = 0$. 
\end{ncorollary}
\begin{nproof}
    The first claim follows by setting $n = 0$ in the previous Theorem, and the second by using that $e^{-x} = \frac{1}{e^x}$. \qed
\end{nproof}
\noindent Since $e^x$ is strictly increasing, we may now define an inverse of $e^x$ at each $y > 0$. 

\subsection{The Logarithm}
\begin{ndef}{: The Logarithm}{}
    For $y > 0$, define $L(y) = \log(y)$ by $E(L(y)) = y$ for $y > 0$, $y \in \RR$. Equivalently, we can define the logarithm as $L(E(x))) = x$ for $x \in \RR$ or $e^{\log y} = y$ or $\log e^x = x$. 
\end{ndef}
\begin{figure}[htbp]
    \centering
    \begin{tikzpicture}[scale=1.5]
        \draw[-latex, very thick] (0, 0) -- (0, 2);
        \draw[latex-latex, very thick] (-2, 0) -- (2, 0);
        \draw[blue, thick, smooth, <->, samples = 100, domain=-1.8:1.25, variable = \x] plot(\x, {0.5*exp(\x)});
        \node[above, text = blue] at (-1.5,0.2) {$e^x$};
        \draw[dashed] (0, 1.39) -- (1, 1.39);
        \draw[dashed] (1, 0) -- (1, 1.39);
        \draw[] (0, 1.39) -- (-0.15, 1.39);
        \node[left] at (-0.15, 1.39) {$y$};
        \draw[] (1, 0) -- (1, -0.15);
        \node[below] at (1, -0.15) {$L(y)$};
    \end{tikzpicture}

    \caption{Plot of $e^x$ and extraction of its inverse $L(y)$.}
    \label{fig53}
\end{figure}

\begin{ntheorem}{: Derivative and Addition Formula}{}
    $L'(y) = \frac{1}{y}$ for $y > 0$ and $L(uv) = L(u) + L(v)$
\end{ntheorem}
\begin{nproof}
    Since $L(E(x)) = x$ by definition, by the chain rule (Theorem \ref{thm:5.5}) we have that:
    \begin{align*}
        L'(E(x))E'(x) = 1 \implies L'(E(x)) = \frac{1}{E(x)}.
    \end{align*}
    Where we use that $E'(x) = E(x)$ in the above implication. Letting $E(x) \mapsto y$, we have that $L'(y) = \frac{1}{y}$ as claimed. For the second claim, consider that for all $u, v > 0$, there exists unique $x, y \in \RR$ (which can be denoted as $\exists ! x, y \in \RR$) such that $E(x) = u, E(y) = v$. Hence:
    \begin{align*}
        L(uv) = L(E(x)E(y)) = L(E(x + y)) = x + y = L(u) + L(v).
    \end{align*}\qed
\end{nproof}

\begin{ncorollary}{}{}
    For $y > 0$, $L(y) = \int_1^y \frac{1}{t}dt$.
\end{ncorollary}
\begin{nproof}
    By the Fundamental Theorem of Calculus (Theorem \ref{thm:6.21}) we have that $\int_1^y \frac{1}{t}dt = L(y) - L(1) = L(y)$. \qed
\end{nproof}

\begin{ntheorem}{}{}
    For $p \in \QQ$, $L(x^p) = pL(x)$.
\end{ntheorem}
\begin{nproof}
    By the addition formula, we have that $L(\frac{1}{x}) + L(x) = L(\frac{1}{x}x) = L(1) = 0$. Hence, $L(\frac{1}{x}) = -L(x)$. Furthermore, by the addition formula, we have that $L(x^n) = nL(x)$ by induction. Furthermore, $L(x^0) = L(1) = 0 = 0L(x)$ which shows that the formula holds for $\NN \cup \set{0}$. Combining the previous facts, we have that:
    \begin{align*}
        nL(x^{\frac{1}{n}}) = L((x^{\frac{1}{n}})^n) = L(x^1) \implies L(x^{\frac{1}{n}}) = \frac{1}{n}L(x)
    \end{align*} 
    We therefore have that for $p = \frac{m}{n}$ with $m, n \in \NN$ that:
    
    \begin{align*}
        L(x^{\frac{m}{n}}) = mL(x^{\frac{1}{n}}) = \frac{m}{n}L(x)
    \end{align*}
    Furthermore,
    \begin{align*}
        L(x^{-\frac{m}{n}}) = L\left(\frac{1}{x^{\frac{m}{n}}}\right) = -L(x^{\frac{m}{n}}) = -\frac{m}{n}L(x)
    \end{align*}
    which shows that the proposed identity holds for all $p \in \QQ$. \qed
\end{nproof}

\begin{ndef}{: Real Exponentials with Arbitrary Base}{}
    For $\alpha \in \RR$, we define $x^\alpha = e^{\alpha \log \alpha}$.
\end{ndef}
\noindent Note that the above definition is equivalen to the definition made in MATH 320 HW3Q1, but it is a much cleaner definition (as we will soon see)!

\begin{ntheorem}{: Generalized Power Rule}{}
    $\od{}{x}x^\alpha = \alpha x^{\alpha -1}$ and hence $x^\alpha$ has antiderivative:
    \begin{align*}
        \begin{cases}
            \frac{1}{\alpha + 1}x^{\alpha + 1} & \text{if $\alpha \neq -1$}
            \\ \log x & \text{if $\alpha = 1$}
        \end{cases}.
    \end{align*}
\end{ntheorem}
\begin{nproof}
    From the definition of $x^\alpha$, we have that:
    \begin{align*}
        \dod{}{x}x^\alpha = \dod{}{x}\left(e^{\alpha \log x}\right) = e^{\alpha \log x} \alpha\frac{1}{x} = \alpha\frac{x^\alpha}{x} = \alpha x^{\alpha - 1}
    \end{align*}
    where in the second equality we use the chain rule and the fact that $L'(y) = \frac{1}{y}$. \qed
\end{nproof}

\begin{ntheorem}{: Subpolynomial Asymptotic Growth}{}
    $\lim_{x \rightarrow \infty} \log x = \infty$, $\lim_{x \rightarrow 0^+} = -\infty$, and $\lim_{x \rightarrow \infty} \frac{\log x}{x^\alpha} = 0$ if $\alpha > 0$.
\end{ntheorem}
\begin{nproof}
    To realize the first two equalities, we first make the observation that $\log(x)$ is (Strictly) monotonically increasing. To see this, let $x_1, x_2 > 0$ and $x_1 < x_2$ and then we have that:
    \begin{align*}
        \log(x_2) - \log(x_1) = \int_0^{x_2} \frac{1}{t}dt - \int_0^{x_1}\frac{1}{t}dt = \int_{x_1}^{x_2}\frac{1}{t}dt > 0
    \end{align*}
    where the bound follows from the fact that $\frac{1}{t} > 0$ for all $t > 0$ and $x_2 - x_1 > 0$. Hence, $\log(x)$ is monotonicaly increasing, and to compute the limits it suffices to compute the limit along a specific choice of sequence that tends to $\infty$ or $0$ respectively. Using that $\linf e^n = \infty$ and $\linf e^{-n} = 0$, we have that:
    \begin{align*}
        \linf \log(e^n) = \linf n\log(e) = \linf n = \infty
    \end{align*}
    \begin{align*}
        \linf \log(e^{-n}) = \linf -n\log(e) = \linf -n = -\infty
    \end{align*}
    so we conclude that $\lim_{x \rightarrow \infty}\log(x) = \infty$ and $\lim_{x \rightarrow 0} \log(x) = -\infty$.

    For the third claim, we let $a > 0$ and $x > 1$. Then:
    \begin{align*}
        \log(x) = \int_1^x \frac{1}{t}dt < \int_1^x t^a\frac{1}{t}dt = \left.\frac{t^a}{a}\right|_{1}^x = \frac{x^a}{a} - \frac{1}{a} < \frac{x^a}{a}
    \end{align*}
    so choosing $a \in (0, \alpha)$ we have that:
    \begin{align*}
        \frac{1}{x^\alpha}\log x < \frac{1}{a}\frac{1}{x^{\alpha - a}} \rightarrow 0 \text{ as } x \rightarrow \infty.
    \end{align*}\qed
\end{nproof}

\subsection{Cosine and Sine}
We have seen that $E(z+w) = E(z)E(w)$ for all $z, w \in \CC$. A natural definition for the complex exponential follows.

\begin{ndef}{: Complex Exponentials}{}
    Given $z \in \CC$, we define:
    \begin{align*}
        e^z = \sum_{n=0}^\infty \frac{z^n}{n!} = E(z)
    \end{align*}
\end{ndef}
\noindent As a remark, note in taking the complex conjugate of $\exp(z)$, we can absorb the conjugation into the argument:
\begin{align*}
    \overline{\exp(z)} = \overline{\sum_{n=0}^\infty \frac{z^n}{n!}} = \sum_{n=0}^\infty \frac{\overline{z}^n}{n!} = \exp(\overline{z})
\end{align*}
\noindent We will now define the trigonometric functions using the complex exponential, and prove the properties that we would expect them to have from our prior geometric notions.

\begin{ndef}{: Cosine and Sine}{}
    Let $x \in \R$. We then define:
    \begin{align*}
        C(x) &= \Re E(ix) = \frac{1}{2}\left[e^{ix} + e^{-ix}\right]
        \\ S(x) &= \Im E(ix) = \frac{1}{2i}\left[e^{ix} - e^{-ix}\right]
    \end{align*}
\end{ndef}

\begin{ntheorem}{: Euler's Formula}{}
    $E(ix) = C(x) + iS(x)$. 
\end{ntheorem}
\begin{nproof}
    The formula is an immediate consequence of the definitions of $C(x), S(x)$. \qed
\end{nproof}
\noindent Note that $C(x), S(x)$ can alternatively (equivalently) be defined as power series:
\begin{align*}
    C(x) &= \frac{1}{2}\sum_{n=0}^\infty \left(\frac{(ix)^n}{n!} + \frac{(-ix)^n}{n!}\right) = \sum_{n=0}^\infty (-1)^n \frac{x^{2n}}{(2n)!}
    \\ S(x) &= \sum_{n=0}^\infty (-1)^n \frac{x^{2n}}{(2n+1)!}
\end{align*}

\begin{ntheorem}{}{}
    Let $x \in \RR$. We then have that:
    \begin{enumerate}
        \item $C(x) = C(-x)$, $C(0) = 1$ and $S(x) = -S(-x)$, $S(0) = 0$.
        \item $C^2(x) + S^2(x) = 1$.
        \item $C'(x) = -S(x)$ and $S'(x) = C(x)$.
    \end{enumerate}
\end{ntheorem}

\begin{nproof}
    \begin{enumerate}
        \item From the definitions of $C$ and $S$, we have:
        \begin{align*}
            C(-x) &= \frac{1}{2}\left[e^{-ix} + e^{ix}\right] = C(x)
            \\ C(0) &= \frac{1}{2}\left[e^{i(0)} + e^{-i(0)}\right] = \frac{1}{2}\left[1 + 1\right] = 1
            \\ -S(-x) &= -\frac{1}{2}\left[e^{-ix} - e^{ix}\right] = \frac{1}{2}\left[e^{ix} - e^{-ix}\right] = S(x)
            \\ S(0) &= \frac{1}{2}\left[e^{i(0)} - e^{-i(0)}\right] = \frac{1}{2}\left[1 - 1\right] = 0
        \end{align*}
        \item Expanding out the expression, we have:
        \begin{align*}
            C^2(x) + S^2(x) &= \frac{1}{4}\left[e^{ix}e^{ix} + 2e^{ix}e^{-ix} + e^{-ix}e^{-ix}\right] - \frac{1}{4}\left[e^{ix}e^{ix} - 2e^{ix}e^{-ix} + e^{-ix}e^{-ix}\right]
            \\ &= e^{ix}e^{-ix}
            \\ &= e^{ix - ix}
            \\ &= e^0
            \\ &= 1
        \end{align*}
        \item Using the linearity of the derivative, and the known result for the derivative of exponentials we have:
        \begin{align*}
            C'(x) &= \frac{1}{2}\left[ie^{ix} - ie^{-ix}\right] = -\frac{1}{2i}\left[e^{ix} - e^{-ix}\right] = -S(x)
            \\ S'(x) &= \frac{1}{2i}\left[ie^{ix} + ie^{-ix}\right] = \frac{1}{2}\left[e^{ix} + e^{-ix}\right] = C(x)
        \end{align*} \qed
    \end{enumerate}
\end{nproof}

\begin{nlemma}{}{}
    There exists $x > 0$ such that $C(x) = 0$. 
\end{nlemma}
\begin{nproof}
    Suppose for the sake of contradiction that $C(x) > 0$ for all $x > 0$. Then, $S$ is strictly increasing as $S' = C$. So, for all $y > x$, we then have that:
    \begin{align*}
        S(x)(y - x) < \int_x^y S(t)dt = -C(y) + C(x) \leq 2
    \end{align*}
    but letting $y \rightarrow \infty$, we get that $\infty \leq 2$ which is a contradiciton. \qed
\end{nproof}

\begin{ndef}{: $\pi$}{}
    Given the above Lemma, $x_0 = \inf\set{x > 0: C(x) = 0}$ exists. In particular, $x_0 > 0$ since $C(0) = 1$ and $C$ is continuous. Then, we define $\pi = 2x_0$. 
\end{ndef}

\begin{ntheorem}{}{}
    $S(\frac{\pi}{2}) = 1$.
\end{ntheorem}
\begin{nproof}
    By the definition of $\pi$, we have that $C(x) > 0$ for $x \in [0, \frac{\pi}{2})$ and $C(\frac{\pi}{2}) = 0$. From the previous theorem we have that $C^2(\frac{\pi}{2}) + S^2(\frac{\pi}{2}) = 1$ so we obtain that $S(\frac{\pi}{2}) = \pm 1$. Since $S(0) = 0$ and $S'(x) = C(x) > 0$ for $x \in [0, \frac{\pi}{2})$, we conclude that $S(\frac{\pi}{2}) = 1$. \qed
\end{nproof}
\noindent Note the implication this result has for $e^{ix}$; using Euler's Formula, we find that:
\begin{align*}
    e^{i\frac{\pi}{2}} = \cos(\frac{\pi}{2}) + i\sin(\frac{\pi}{2}) = 0 + i1 = i
\end{align*}
Therefore:
\begin{align*}
    e^{i\pi} &= \left(e^{i\frac{\pi}{2}}\right)^2 = i^2 = -1
    \\ e^{i\frac{3\pi}{2}} &= \left(e^{i\frac{\pi}{2}}\right)^3 = i^3 = -i
    \\ e^{i2\pi} &= \left(e^{i\frac{\pi}{2}}\right)^4 = i^4 = 1.
\end{align*}
From this we notice the periodicity of $e^{ix}$. 
\begin{ntheorem}{: Periodicity of Trigonometric Functions}{}
    \begin{enumerate}
        \item $e^{x + 2\pi i} = e^x$.
        \item $C(x + 2\pi) = C(x)$.
        \item $S(x + 2\pi) = S(x)$.
    \end{enumerate}
\end{ntheorem}
\begin{nproof}
    \begin{enumerate}
        \item $e^{x + 2\pi i} = e^{x}e^{2\pi i}$ by the addition formula. Then, $e^{2\pi i} = 1$ by the argument above, proving the identity.
        \item Using the above periodicity of $e^{ix}$, we have that $C(x + 2\pi) = \Re(e^{i(x + 2\pi)}) = \Re(e^{ix}) = C(x)$.
        \item $S(x + 2\pi) = \Im(e^{i(x + 2\pi)}) = \Im(e^{ix}) = C(x)$. \qed
    \end{enumerate}
\end{nproof}
\noindent Note that there is a way to relate $S$ and $C$ via a phase shift. We observe that:
\begin{align*}
    S(x + \frac{\pi}{2}) = \Im(e^{i(x + \frac{\pi}{2})}) = \Im(e^{ix}e^{i\frac{\pi}{2}}) = \Im(e^{ix}i) = \Im((C(x) + iS(x))i) = \Im(iC(x) - S(x)) = C(x)
\end{align*}
\noindent We can also generalize this formula to get the familiar trigonometric sum identity.

\begin{ntheorem}{}{}
    $S(x + y) = C(x)S(y) + S(x)C(y)$.
\end{ntheorem}
\begin{nproof}
    Using the definition of $S$ and Euler's Formula, we observe that:
    \begin{align*}
        S(x + y) = \Im(e^{i(x + y)}) 
        &= \Im(e^{ix}e^{iy})
        \\ &= \Im((C(x) + iS(x))(C(y) + iS(y)))
        \\ &= \Im(C(x)C(y) + iS(x)C(y) + iC(x)S(y) - S(x)S(y))
        \\ &= C(x)C(y) - S(x)S(y)
    \end{align*} \qed
\end{nproof}
\noindent As a final remark before moving onto the next section, we observe that $x \mapsto e^{ix}$ is a bijection from $[0, 2\pi)$ onto the unit circle (points $z \in \CC$ with $\abs{z} = 1$). See Rudin for more details.
\begin{figure}[htbp]
    \centering
    \begin{tikzpicture}
        \draw[blue, thick] (0, 0) -- (1, 1);
        \draw[latex-latex, very thick] (-2, 0) -- (2, 0);
        \draw[latex-latex, very thick] (0, -2) -- (0, 2);
        \draw[smooth] (0, 0) circle (40pt);
        \draw[fill = blue, draw = blue] (1, 1) circle (1pt);
        \draw[->,red] (0.75, 0) arc(0:45:0.75);
        \node[text = red] at (0.82, 0.37) {$x$};
        \node[text = blue] at (1.23, 1.23) {$e^{ix}$};

    \end{tikzpicture}
    
    \caption{Visualization of the map $x \mapsto e^{ix}$ from $[0, 2\pi)$ onto the unit circle in the complex plane.}
    \label{fig55}
\end{figure}


\subsection{The Algebraic Completeness of the Complex Field}

\setcounter{rudin}{7}

\begin{theorem}{The Fundamental Theorem of Algebra}{}
    Let $n \in \NN$, and $a_0, a_1, \ldots, a_n \in \CC$ such that $a_n \neq 0$. Define the polynomial:
    \begin{align*}
        P(z) = a_0 + a_1z + \ldots + a_nz^n
    \end{align*}
    with $z \in \CC$. Then, there exists $z_0 \in \CC$ such that $P(z_0) = 0$. 
\end{theorem}

\begin{ncorollary}{}{}
    By division, $P_n(z)$ has $n$ roots. 
\end{ncorollary}

\begin{nproof}
    Assume WLOG that $a_n = 1$ (this can be realized by dividing out by the original nonzero $a_n$). Let $\mu = \inf_{z \in \CC}\abs{P(z)}$. We wish to show that $\mu = 0$, and that the inf is attained at some $z_0 \in \CC$. 

    We first show that the infimum is attained. The idea of the argument is that for large $z$, $z^n$ grows the most rapidly and hence it dominates. Hence, $\abs{P(z)}$ is large. Hence, the $\inf$ is attained in some compact disk, but since $P$ is continuous, the $\inf$ is therefore obtained somewhere on this compact set. Formally, for $\abs{z} = R$, we have that:
    \begin{align*}
        \abs{P(z)} = \abs{z^n}\abs{\frac{a_0}{z^n} + \frac{a_1}{z^{n-1}} + \ldots + \frac{a_{n-1}}{z} + 1} &\geq \abs{z}^n\left(1 - \frac{\abs{a_0}}{\abs{z}^n} - \frac{\abs{a_1}}{\abs{z}^{n-1}} - \ldots - \frac{\abs{a_{n-1}}}{\abs{z}}\right)
        \\ &= R^n\left(1 - \frac{\abs{a_0}}{\abs{z}^n} - \frac{\abs{a_1}}{\abs{z}^{n-1}} - \ldots - \frac{\abs{a_{n-1}}}{\abs{z}}\right)
    \end{align*}
    We see that this expression goes to infinity as $R \rightarrow \infty$. Hence, $\abs{P(z)} \geq \mu + 1$ if $\abs{z} \geq R_0$ for some $R_0 > 0$. As $\abs{P}$ is continuous, and $\abs{z} \leq R_0$ is a compact subset of $\CC$, $\mu$ is attained somewhere on the subset by the Extreme Value Theorem (Theorem \ref{thm:4.16}). Therefore, $\mu = \abs{P(z_0)}$ for some $z_0$ with $\abs{z_0} \leq R_0$. 

    Next, we show that $\mu = 0$. Suppose (for the sake of contradiction) that $\mu > 0$ and hence $P(z_0) \neq 0$. Let $Q(z) = \frac{P(z_0 + z)}{P(z_0)}$. Then $Q(0) = 1$, so:
    \begin{align*}
        Q(z) = 1 + b_kz^k + \ldots + b_nz^n
    \end{align*}
    where $b_k$ is the first nonzero coeffient. Furthermore, we see that $\abs{Q(z)} = \frac{\abs{P(z_0 + z)}}{\mu} \geq 1$ for all $z \in \CC$ (as $\mu$ is the infimum). We will now derive a contradiction by looking at small $z$ (where $z^k > z^{k+1} > \ldots > z^n$). We consider that:
    \begin{align*}
        \abs{Q(z)} \leq \abs{1 + b_kz^k} + \sum_{m=k+1}^n \abs{b_m}\abs{z^m}
    \end{align*}
    We want to choose $z$ small enough such that the first term in the above expression is less than $1$ and the others are negligeble. To this end, let us write $bk = \frac{b_k}{\abs{b_k}}\abs{b_k} = e^{it_1}\abs{b_k}$ for some $t_1 \in \RR$. Then, let $t = \frac{-t_1 + \pi}{k}$ so $t_1 = -kt + \pi$. We then have that $b_k = -e^{-itk}\abs{b_k}$ (where the minus sign comes from $e^{i\pi}$). Choose $z = re^{it}$ with $r > 0$. Then, $z^k = r^ke^{itk}$ and $b_kz^k = -e^{-itk}\abs{b_k}r^ke^{itk} = -r^k\abs{b_k} < 0$. Let us choose $r$ small enough so that $\abs{b_k}r^k < 1$ is satisfied. We then have that:
    \begin{align*}
        \abs{1 + b_kz^k} = \abs{1 - \abs{b_k}r^k} = 1 - \abs{b_k}r^k
    \end{align*}
    so therefore:
    \begin{align*}
        \abs{Q(z)} \leq 1 - \abs{b_k}r^k + \sum_{m=k+1}^n \abs{b_m}r^k = 1 - r^k\left(\abs{b_k} - r\abs{b_{k+1}} - \ldots - r^{n-k}\abs{b_n}\right)
    \end{align*}
    where $\left(\abs{b_k} - r\abs{b_{k+1}} - \ldots - r^{n-k}\abs{b_n}\right) > 0$. Hence $\abs{Q(z)} < 1$, which is a contradiction. We conclude that $\abs{P(z_0)} = 0$. \qed
\end{nproof}

\subsection{Fourier Series}
\noindent We now begin our discussion on Fourier Series. Note that the theory of Fourier Series (and more generally, Harmonic Analysis) is rich enough for it to be its own course, but we will here give an introduction to the topic. Fourier Series show up everywhere, such as (for example) in partial differential equations, or in signal processing. It is an interesting topic of study that combines analysis and linear algebra. 

\begin{ndef}{: Inner Product of Functions}{}
    Given $a, b \in \RR$ with $a < b$ and integrable $f, g: [a, b] \mapsto \CC$ we write:
    \begin{align*}
        \avg{f, g} = \int_a^b f(x)\overline{g(x)}dx \; \left(= \overline{\avg{g, f}}\right)
    \end{align*}
    to denote the inner product on the space of complex functions. Note that this inner product induces the norm $\norm{f}_2 = \sqrt{\avg{f, f}} = \left(\int_a^b \abs{f(x)}^2 dx\right)^{1/2}$. 

    Note that we will often take $[a, b] = [0, 2\pi]$ and in this case we may prefer:
    \begin{align*}
        \avg{f, g} = \frac{1}{2\pi}\int_0^{2\pi}f(x)\overline{g(x)}dx.
    \end{align*} 
\end{ndef}
\noindent As a remark, we have that $d(f, g) = \norm{f - g}_2$ induces a metric, but we have to be careful; it defines a metric on the metric space of continuous functions (that is, $\C[a, b]$) but \emph{not} as on the space of Riemann-integrable functions (i.e. $\R[a, b]$). To see this, consider that we can have $f \in \R[a, b]$ with $\int_a^b \abs{f(x)}dx = 0$ but $f \neq 0$ (consider any function $f$ which is zero everywhere but is nonzero for a finite number of points). 

\begin{ndef}{: Orthogonal/Orthonormal Families of Functions}{}
    We say that a family of functions $\phi_n [a, b] \mapsto \CC$ are mutually \textbf{orthogonal} if:
    \begin{align*}
        \avg{\phi_n, \phi_m} = 0 \text{ if $n \neq m$.}
    \end{align*}
    We say that a family of functions is \textbf{orthonormal} if:
    \begin{align*}
        \avg{\phi_n, \phi_m} = \delta_{nm} = \begin{cases}
            1 & n = m
            \\ 0 & n \neq m
        \end{cases}.
    \end{align*}
\end{ndef}

\begin{nexample}{}{}
    \begin{enumerate}
        \item Let $[a, b] = [-\pi, \pi]$. Then, $\phi_n(x) = \frac{1}{\sqrt{2\pi}}e^{inx}$, $n \in \ZZ$ obey:
        \begin{align*}
            \avg{\phi_n, \phi_m} = \frac{1}{2\pi}\int_{-\pi}^\pi e^{i(m-n)x}dx = \frac{1}{2\pi}\int_{-\pi}^\pi\cos((m-n)x) + i\sin((m-n)x)dx = \delta_{nm}.
        \end{align*}
        \item Take $\phi_1, \phi_2, \ldots$ to be $\frac{1}{\sqrt{2\pi}}, \frac{1}{\sqrt{2\pi}}\cos(x),  \frac{1}{\sqrt{2\pi}}\sin(x),  \frac{1}{\sqrt{2\pi}}\cos(2x), \ldots$. We then have that this family is orthonormal on $[-\pi, \pi]$. 
        \item The Legendre polynomials, defined by $P_n(x) = \sum_{k=0}^n \binom{n}{k}\binom{n+k}{n}\left(\frac{x - 1}{2}\right)^k$ for $n \in \NN \cup \set{0}$ are orthogonal on $[-1, 1]$. Note that $\norm{P_n}_2 = \sqrt{\frac{2}{2n+1}}$. 
    \end{enumerate}
\end{nexample}
\noindent With this definition defined, we will want to use sets of orthonormal functions as a \emph{basis} of $L^2$ space. We want to be able to write arbitrary $f \in L^2$ as a linear combination of these functions.

\begin{nexample}{}{}
    As motivation for the next definition, suppose $f(x) = \sum_{m=0}^N c_n \phi_n$ with $\set{\phi_n}$ orthonormal on $[a, b]$. Then, we can write:
\begin{align*}
    \avg{f, \phi_n} = \sum_{m=0}^N c_m \avg{\phi_m, \phi_n} = \sum_{m=0}^NN c_m \delta_{mn} = c_n.
\end{align*}
That is, we can write $c_n = \avg{f, \phi_n} = \int_a^b f(x)\overline{\phi_n(x)}dx$.

As a specific example, suppose $f(x) = \sum_{n=-N}^N c_ne^{inx} = \sum_{n=-N}c_n\sqrt{2\pi}\frac{e^{inx}}{\sqrt{2\pi}}$. From the previous example, we have that $\frac{e^{inx}}{\sqrt{2\pi}}$ is orthonormal on $[-\pi, \pi]$. Hence:
\begin{align*}
    \sqrt{2\pi}c_n = \avg{f, \phi_n} = \frac{1}{\sqrt{2\pi}}\int_{-\pi}^\pi f(x)e^{inx}.
\end{align*}
That is, $c_n = \frac{1}{2\pi}\int_{-\pi}^\pi f(x)e^{-inx}dx$. 
\end{nexample}

\begin{ndef}{: Fourier Coefficients/Series}{}
    If $f \in \R[a, b]$ and $\set{\phi_n}$ is an orthonormal family of functions, then $c_n = \avg{f, \phi_n}$ is called a \textbf{Fourier coefficient of $f$}, and the \textbf{Fourier series of $f$} is $\sum_n c_n \phi_n$. 
\end{ndef}
\noindent A natural question after making this definition is ``when does the Fourier series of $f$ converge?'' A follow-up question is ``when it converges, is it equal to $f$?'' We will investigate these questions with the next sequence of theorems.

\setcounter{rudin}{10}

\begin{theorem}{}{}
    Suppose $f \in \R[a, b]$ and $\set{\phi_n}$ is an orthonormal on $[a, b]$. Let $c_n = \avg{f, \phi_n}$ and $s_n = \sum_{m=1}^n c_m\phi_m$. Let $t_n = \sum_{m=1}^n a_m\phi_m$ for some $a_m \in \CC$. Then, we have that:
    \begin{align*}
        \norm{f- s_n}_2^2 \leq \norm{f - t_n}_2^2
    \end{align*}
    with equality if and only if $c_m = a_m$ for each $m$. 
\end{theorem}
\noindent The moral of the theorem is that if we are to use a linear combination of the first $n$ functions out of an orthonormal set of functions to best approximate $f$ in the $L^2$ norm, the best way to do so is by using Fourier coefficients. 
