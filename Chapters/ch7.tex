\section[Sequences and Series of Functions]{\hyperlink{toc}{Sequences and Series of Functions}}

\subsection{Motivating Examples}
\begin{nexample}{}{}
    For $m, n \in \NN$, let $p_{n, m} = \frac{m}{n}$. Then, 
    \begin{align*}
        \lim_{m\rightarrow \infty} p_{m, n} = \infty, \quad \linf p_{m, n} = 0
    \end{align*}
    In particular,
    \begin{align*}
        \lim_{m \rightarrow \infty}\linf p_{m, n} = 0, \quad \linf\lim_{m \rightarrow \infty} p_{m, n} = \infty.
    \end{align*}
    Which demonstrates that the order of which limits are taken in can affect the value.
\end{nexample}

\begin{nexample}{}{}
    Define the sequence of functions:
    \begin{align*}
        f_n(x) = \begin{cases}
            1 & x \geq 0
            \\ 1 + nx & -\frac{1}{n} < x < 0
            \\ 0 & x \leq -\frac{1}{n}
        \end{cases}
    \end{align*}
    Since $f_n$ is piecewise linear, it is continuous. However, looking at the $n \rightarrow \infty$ limit, we have:
    \begin{align*}
        \linf f_n(x) = \begin{cases}
            1 & x \geq 0
            \\ 0 & x < 0
        \end{cases}
    \end{align*}
    Which is the right continuous step function, which is evidently discontinuous at $x = 0$. Hence, the limit of continuous functions can be discontinuous. Another way of viewing this problem is:
    \begin{align*}
        \linf \lim_{x \rightarrow 0} f_n(x) = 0, \quad \lim_{x \rightarrow 0} \linf f_n(x) = \text{D.N.E.}
    \end{align*}
    so again we see the order of taking our limits can be important.
\end{nexample}
\begin{figure}[htbp]
    \centering
    \begin{tikzpicture}[scale=1.5]
        \draw[latex-latex, very thick] (-2, 0) -- (2, 0);
        \draw[-latex , very thick] (0, 0) -- (0, 2);
        \draw[<-, thick, blue] (-1.5, 0) -- (-0.5, 0);
        \draw[thick, blue] (-0.5, 0) -- (0, 1);
        \draw[->, thick, blue] (0,1) -- (1.5, 1);
        \draw[] (-0.5, 0) -- (-0.5, -0.15);
        \node[below] at (-0.5, -0.15) {$-\frac{1}{n}$};
    \end{tikzpicture}
    
    
    \caption{Plot of $f_n$ in the above example.}
    \label{fig37}
\end{figure}

\setcounter{rudin}{3}

\begin{example}{}{7.4}
    For $m \in \NN$ and $x \in \RR$, let $f_m(x) = \lim_{n \rightarrow \infty} \left[\cos(m!\pi x)\right]^{2n}$. Since $\abs{\cos(k\pi)} = 1$ if $k \in \ZZ$, we see that $f_m(x) = 1$ when $m! x \in \ZZ$. Conversely, since $\abs{\cos(k\pi)} < 1$ if $k \neq \ZZ$, $f_m(x) = 0$ when $m! x \notin \ZZ$. Some plots of $f_m(x)$ on $[0, 1]$ for $m = 1, 2, 3$ are below as a visualization. We now define $f(x) = \lim_{m \rightarrow \infty} f_m(x)$. If $x = \frac{p}{q} \in \QQ$, then $m! x = \frac{m! p}{q} \in \ZZ$ for $m$ large enough (for $m \geq q$, as the denominator cancells). Therefore, we have that $f(x) = 1$ for $x \in \QQ$. Conversely, if $x \notin \QQ$, then $m! x \notin \ZZ$ for all $m \in \NN$. So, $f_m(x) = 0$ for all $m$, and $f(x) = 0$. Therefore, we have that:
    \begin{align*}
        f(x) = \lim_{m \rightarrow \infty} f_m(x) = \begin{cases}
            1 & x \in \QQ
            \\ 0 & x \notin \QQ
        \end{cases}.
    \end{align*}
    In other words, $f$ is the Dirchlet function. The interesting part is that each of the $f_m(x)$ are Riemann integrable on $[0, 1]$ by Theorem \ref{thm:6.10} (as $f$ has finitely many discontinuities for any $m \in \NN$). However, the limit is not Riemann integrable, as we prove below. Hence, the limit of Riemann integrable functions is not necessarily Riemann integrable.
\end{example}

\begin{figure}[htbp]
    \centering
    \begin{tikzpicture}[scale = 1.5]
        \draw[-latex, very thick] (0, 0) -- (0, 2);
        \draw[-latex, very thick] (0, 0) -- (2, 0);
        \draw[-latex, very thick] (2.5, 0) -- (2.5, 2);
        \draw[-latex, very thick] (2.5, 0) -- (4.5, 0);
        \draw[-latex, very thick] (5, 0) -- (5, 2);
        \draw[-latex, very thick] (5, 0) -- (7, 0);
        \filldraw[blue] (0, 1) circle (1.5pt);
        \filldraw[blue] (1.5, 1) circle (1.5pt);
        \draw[thick, blue] (0, 0) -- (1.5, 0);
        \draw[blue, fill = white] (0, 0) circle (1.5pt);
        \draw[blue, fill = white] (1.5, 0) circle (1.5pt);
        \node[text = blue] at (0.75, 0.5) {$f_1(x)$};

        \filldraw[blue] (2.5, 1) circle (1.5pt);
        \filldraw[blue] (3.25, 1) circle (1.5pt);
        \filldraw[blue] (4, 1) circle (1.5pt);
        \draw[thick, blue] (2.5, 0) -- (4, 0);
        \draw[blue, fill = white] (2.5, 0) circle (1.5pt);
        \draw[blue, fill = white] (3.25, 0) circle (1.5pt);
        \draw[blue, fill = white] (4, 0) circle (1.5pt);
        \node[text = blue] at (3.25, 0.5) {$f_2(x)$};

        \filldraw[blue] (5, 1) circle (1.5pt);
        \filldraw[blue] (5.25, 1) circle (1.5pt);
        \filldraw[blue] (5.5, 1) circle (1.5pt);
        \filldraw[blue] (5.75, 1) circle (1.5pt);
        \filldraw[blue] (6, 1) circle (1.5pt);
        \filldraw[blue] (6.25, 1) circle (1.5pt);
        \filldraw[blue] (6.5, 1) circle (1.5pt);
        \draw[thick, blue] (5, 0) -- (6.5, 0);
        \draw[blue, fill = white] (5, 0) circle (1.5pt);
        \draw[blue, fill = white] (5.25, 0) circle (1.5pt);
        \draw[blue, fill = white] (5.5, 0) circle (1.5pt);
        \draw[blue, fill = white] (5.75, 0) circle (1.5pt);
        \draw[blue, fill = white] (6, 0) circle (1.5pt);
        \draw[blue, fill = white] (6.25, 0) circle (1.5pt);
        \draw[blue, fill = white] (6.5, 0) circle (1.5pt);
        \node[text = blue] at (5.75, 0.5) {$f_3(x)$};
    \end{tikzpicture}
    \caption{Plot of $f_m(x)$ over the interval $[0, 1]$ for $m = 1, 2, 3$. For $m = 1$, only $x = 0, 1$ satisfy $m! x = x \in \ZZ$. For $m = 2$, we have that $x = 0, \frac{1}{2}, 1$ satisfy $m!x = 2x \in \ZZ$. Finally, for $m = 3$, we have that $x = 0, \frac{1}{6}, \frac{2}{6}, \frac{3}{6}, \frac{4}{6}, \frac{5}{6}, 1$ satisfy $m!x = 6x \in \ZZ$.}
    \label{fig38}
\end{figure}

\noindent We now show that $f$ defined in the above example is not Riemann integrable on $[0, 1]$.

\begin{proof}
    Consider any partition $P$ of $[0, 1]$. Due to the density of rational and irrational numbers in $\RR$ (Theorem \ref{thm:1.20}) we have that $M_i = \sup{f(x): x \in [x_{i-1}, x_i]} = 1$ and $m_i = \inf{f(x): x \in [x_{i-1}, x_i]} = 0$ for all $i$. Therefore, we have that $U(P, f) = \sum_{i=1}^N M_i \Delta x_i = 1$ and $L(P, f) = \sum_{i=1}^N m_i \Delta x_i = 0$ for all partitions $P$. Therefore, $\sup_P U(P, f) = 1$ and $\inf_P L(P, f) = 0$, and we conclude that $f$ is not Riemann integrable on $[0, 1]$.
\end{proof}


\begin{nexample}{}{}
    Define $f_n$ such that:
    \begin{align*}
        f_n(x) = \begin{cases}
            0 & \abs{x} \geq \frac{1}{n}
            \\ n(nx+1) & -\frac{1}{n} < x < 0
            \\ -n(nx+1) & 0 < x < \frac{1}{n}
            \\ 0 & x = 0
        \end{cases}
    \end{align*}
    Then, we have that $f(x) = \linf f_n(x) = 0$ for all $x$.  Furthermore, we have that $\int_{-1}^1 f_n(x)dx = 1$ for all $n$, but $\int_{-1}^1 f(x)dx = 0$. Hence, we have that:
    \begin{align*}
        \linf \int_{-1}^1 f_n(x)dx = 1 \neq 0 = \int_{-1}^1 \linf f_n(x) dx
    \end{align*}
    showing that problems can arise when we interchange the order of an integral with a limit.
\end{nexample}

\begin{figure}[htbp]
    \centering
    \begin{tikzpicture}[scale=1.5]
        \draw[latex-latex, very thick] (-2, 0) -- (2, 0);
        \draw[-latex , very thick] (0, 0) -- (0, 2);
        \draw[<-, thick, blue] (-1.5, 0) -- (-0.5, 0);
        \draw[thick, blue] (-0.5, 0) -- (0, 1);
        \draw[thick, blue] (0, 1) -- (0.5, 0);
        \draw[->, thick, blue] (0.5, 0) -- (1.5, 0);
        \filldraw[blue] (0, 0) circle (1.5pt);
        \draw[blue, fill = white] (0, 1) circle (1.5pt);
        \node[right] at (0, 1) {$n$};
        \draw[] (-0.5, 0) -- (-0.5, -0.15);
        \node[below] at (-0.5, -0.15) {$-\frac{1}{n}$};
        \draw[] (0.5, 0) -- (0.5, -0.15);
        \node[below] at (0.5, -0.15) {$\frac{1}{n}$};
    \end{tikzpicture}
    
    \caption{Plot of $f_n$ in the above example.}
    \label{fig39}
\end{figure}

\begin{example}{}{7.5}
    Let $f_n(x) = \frac{\sin nx}{\sqrt{n}}$ for $n \in \NN, x \in \RR$. Then, let $f(x) = \linf f_n(x) = 0$ for all $x \in \RR$, so $f'(x) = 0$. However, $f'_n(x) = \frac{1}{\sqrt{n}}n \cos n x = \sqrt{n} \cos n x$ and $\linf \sqrt{n} \cos n x$ does not exist. For example, $f_n'(\pi) = \sqrt{n}(-1)^n$ which is a divergent sequence. So:
    \begin{align*}
        f'(\pi) = \left(\linf f_n\right)'(\pi) = 0 \neq \linf f_n'(\pi)
    \end{align*}
    whcih shows us that problems can arise when interchanging a derivative (which is just a type of limit) with a limit.
\end{example}
\noindent With the above five examples, we have seen examples of bad behaviour that can occur under interchange of limits. Namely:
\begin{enumerate}[1.]
    \item An interchange of the order of limits can change the limiting value for a double sequence.
    \item The limit of a sequence of continuous functions is not necessarily continuous.
    \item The limit of a sequence of Riemann integrable functions is not necessarily Riemann integrable.
    \item The limit of a sequence of Riemann integrals can differ from the Riemann integral of the limit of a sequence.
    \item The limit of a sequence of derivatives can differ from the derivative of a limit of a sequence.
\end{enumerate}

\noindent The good news is that in all of these examples, the sequences we looked at had a ``weak'' form of convergence, where we fix $x$ and then take the $n \rightarrow \infty$ limit. We will now proceed to look at a stronger version of convergence, which looks at ``all $x$ at once'', ensuring that this bad behaviour does not (for the most part) occur.

\subsection{Uniform Convergence}

\setcounter{rudin}{6}
\begin{definition}{Uniform Convergence}{7.7}
    Let $E$ be any set and $f_n: E \mapsto \RR$ or $f_n: E \mapsto \CC$ for $n \in \NN$. Then, $f_n$ \textbf{converges uniformly} to $f$ on $E$ if for all $\e > 0$, there exists $N$ such that $n \geq N$ implies that $\abs{f_n(x) - f(x)} < \e$ for all $x \in E$. 
\end{definition}
\noindent Note the lack of $x$ dependence in the above definition. We give a useful visual intuition of uniform convergence below:

\begin{figure}[htbp]
    \centering
    \begin{tikzpicture}[scale=2]
        \draw[-latex, very thick] (0, 0) -- (2, 0);
        \draw[-latex, very thick] (0, 0) -- (0, 2);
        \node[] at (0.5, 0) {$[$};
        \node[] at (1.5, 0) {$]$};
        \node[below] at (1, 0) {$E$};
        \draw[red] (1, 1.5) parabola (0.5, 0.5);
        \draw[red] (1, 1.5) parabola (1.5, 0.5);
        \draw[red, dotted, yshift = 10pt] (1, 1.5) parabola (0.5, 0.5);
        \draw[red, dotted, yshift = 10pt] (1, 1.5) parabola (1.5, 0.5);
        \draw[red, dotted, yshift = -10pt] (1, 1.5) parabola (0.5, 0.5);
        \draw[red, dotted, yshift = -10pt] (1, 1.5) parabola (1.5, 0.5);
        \draw[<->] (1, 1.5) -- (1, 1.15);
        \node[left] at (1, 1.3) {$\e$};
        \draw[<->] (1, 1.5) -- (1, 1.85);
        \node[left] at (1, 1.625) {$\e$};
        \node[right, text = red] at (1.5, 0.5) {$f$};
    \end{tikzpicture}
    
    \caption{Visualization of the intuition behind uniform convergence. If $f_n \rightarrow f$, uniformly, for any $\e > 0$, we can find $N$ such that for $n \geq N$, $f_n(x)$ lies in the $\e$-tube (pictured above) around $f$.}
    \label{fig40}
\end{figure}

\begin{nexample}{}{}
    Let us return to Example \ref{exam:7.5}. We have that:
    \begin{align*}
        \abs{f_n(x) - f(x)} = \abs{\frac{\sin n x}{\sqrt{n}} - 0} \leq \frac{1}{\sqrt{n}}
    \end{align*}
    So taking $n$ large enough such that $\frac{1}{\sqrt{n}} < \e$, we can see that $f_n(x)$ converges uniformly to $f(x) = 0$. Note that this example does show that uniform convergence is \textit{not} sufficient for:
    \begin{align*}
        \lim_{n \rightarrow \infty} f_n' = \left(\lim_{n \rightarrow \infty} f_n \right)'
    \end{align*}
    to hold. We will return to the relation of uniform convergence and differentiation in a later theorem.
\end{nexample}

\begin{nexample}{}{}
    Let us return to our second example from our section on motivating examples. Recall we had:
    \begin{align*}
        f_n(x) = \begin{cases}
            1 & x \geq 0
            \\ 1 + nx & -\frac{1}{n} < x < 0
            \\ 0 & x \leq -\frac{1}{n}
        \end{cases} \quad f(x) = \begin{cases}
            1 & x \geq 0
            \\ 0 & x < 0
        \end{cases}
    \end{align*}
    We then have that:
    \begin{align*}
        f_n(x) - f(x) = \begin{cases}
            1 + nx & -\frac{1}{n} < x < 0
            \\ 0 & \text{otherwise}
        \end{cases}
    \end{align*}
    So for $x = -\frac{1}{2n}$, we have that:
    \begin{align*}
        f_n\left(-\frac{1}{2n}\right) - f\left(-\frac{1}{2n}\right) = 1 + n\left(-\frac{1}{2n}\right) - 0 = \frac{1}{2}
    \end{align*}
    Which will never be less than $\e$ for $\e < \frac{1}{2}$. Hence, we conclude that $f_n$ does not converge uniformly to $f$ on $\RR$. 
\end{nexample}
\begin{figure}[htbp]
    \centering
    \begin{tikzpicture}[scale=1.5]
        \draw[latex-latex, very thick] (-2, 0) -- (2, 0);
        \draw[-latex , very thick] (0, 0) -- (0, 2);
        \draw[<-, thick, blue] (-1.5, 0) -- (-0.5, 0);
        \draw[thick, blue] (-0.5, 0) -- (0, 1);
        \draw[->, thick, blue] (0,1) -- (1.5, 1);
        \draw[] (-0.5, 0) -- (-0.5, -0.15);
        \node[below] at (-0.5, -0.15) {$-\frac{1}{n}$};
        \draw[->, red] (0, 1) -- (1.5, 1);
        \draw[<-, red] (-1.5, 0) -- (0, 0);
        \draw[red, fill = red] (0, 1) circle (1pt);
        \draw[red, fill = white] (0, 0) circle (1pt);
        \node[left, text = blue] at (-0.25, 0.5) {$f_n$};
        \node[right, text = red] at (1.5, 1) {$f$};
        \draw[dotted, red] (-1.5, 0.1) -- (0, 0.1);
        \draw[dotted, red] (-1.5, -0.1) -- (0, -0.1);
        \draw[dotted, red] (0, 1.1) -- (1.5, 1.1);
        \draw[dotted, red] (0, 0.9) -- (1.5, 0.9);
    \end{tikzpicture}
    
    \caption{Visualization of why the convergence of $f_n \rightarrow f$ in the above example is not uniform. We can see that if we draw a small enough $\e$ tube (i.e. $\e \leq 1$), there is no way to choose $n$ large enough to make all of $f_n(x)$ lie in the tube.}
    \label{fig41}
\end{figure}
\begin{theorem}{Cauchy Criterion for Uniform Convergence}{7.8}
    $f_n$ converges uniformly on $E$ if and only if for all $\e > 0$, thre exists $N$ such that if $m, n \geq N$, then $\abs{f_m(x) - f_n(x)} < \e$ for all $x \in E$. 
\end{theorem}
\noindent Again, note the lack of $x$ dependence in the above theorem.
\begin{nproof}
    $\boxed{\implies}$ Suppose $f_n \rightarrow f$ uniformly on $E$. Then, there exists some $N$ such that for $m, n \geq N$:
    \begin{align*}
        \abs{f_m(x) - f(x)} < \frac{\e}{2}, \quad \abs{f_n(x) - f(x)} < \frac{\e}{2}
    \end{align*}
    for all $x \in E$. Therefore by the triangle inequality, we have that:
    \begin{align*}
        \abs{f_m(x) - f_n(x)} \leq \abs{f_m(x) - f(x)} + \abs{f(x) - f_n(x)} < \frac{\e}{2} + \frac{\e}{2} = \e
    \end{align*}
    Hence $\abs{f_m(x) - f_n(x)} < \e$ for all $x \in E$. 

    $\boxed{\impliedby}$ Let $x \in E$. By assumption, $\set{f_n(x)}_{n \in \NN}$ is a Cauchy sequence, and hence has a limit $f(x)$ (as both $\RR$ and $\CC$, the possible codomains of $f$, are complete). We then let $f(x) = \linf f_n(x)$, so we have pointwise convergence. To see that the convergence is uniform, let $\e > 0$. We know that $\abs{f_m(x) - f_n(x)} < \e$ for $m, n \geq N$ and for all $x$. Then, let $m \rightarrow \infty$. Then, $\abs{f(x) - f_n(x)} \leq \e$. for all $n \geq N$ and all $x \in E$, so the convergence is uniform. \qed
\end{nproof}

\begin{theorem}{}{7.9}
    Suppose $\linf f_n(x) = f(x)$ for $x \in E$, and let:
    \begin{align*}
        M_n = \sup_{x \in E}\abs{f_n(x) - f(x)}
    \end{align*}
    Then, $f_n \rightarrow f$ uniformly on $E$ if and only if $M_n \rightarrow 0$ as $n \rightarrow \infty$.
\end{theorem}
\begin{nproof}
    $\boxed{\implies}$ suppose $f_n \rightarrow f$ uniformly. Then, for any $\e > 0$, there exists some $N \in \NN$ such that for all $n \geq N$ and all $x \in E$:
    \begin{align*}
        \abs{f_n(x) - f(x)} < \e
    \end{align*}
    Since this holds for all $x \in E$, taking the supremum of $\abs{f_n(x) - f(x)}$ we have that:
    \begin{align*}
        \sup_{x \in E}\abs{f_n(x) - f(x)} = M_n \leq \e
    \end{align*}
    We then have that $M_n \leq \e$ for $n \geq N$ for some $N$, and hence $M_n \rightarrow 0$.
    
    $\boxed{\impliedby}$ Suppose that $M_n \rightarrow 0$. Then, for any $\e > 0$, there exists some $N \in \NN$ such that for all $n \geq N$:
    \begin{align*}
        \sup_{x \in E}\abs{f_n(x) - f(x)} = M_n < \e
    \end{align*}
    We then have that for any $x \in E$:
    \begin{align*}
        \abs{f_n(x) - f(x)} \leq \sup_{x \in E}\abs{f_n(x) - f(x)} < \e
    \end{align*}
    so we conclude that $f_n \rightarrow f$ uniformly. \qed
\end{nproof}

\begin{ndef}{: Uniform Convergence of Series}{}
    We say that $\sum_{n = 1}^\infty f_n(x)$ \textbf{converges uniformly} on $E$ if $S_n(x) = \sum_{i=1}^n f_i(x)$ is a uniformly convergent sequence of functions.
\end{ndef}

\begin{theorem}{Weierstrauss M-Test}{7.10}
    Suppose $\abs{f_n(x)} < M_n$ for all $n \geq N_0$ and for all $x \in E$. Suppose also that $\sum_{n = N_0}^\infty M_n < \infty$. Then, $\sum_{n=1}^\infty f_n(x)$ converges uniformly on $E$. 
\end{theorem}
\begin{nproof}
    Let $S_n(x) = \sum_{i=1}^n f_i(x)$. For $n > m \geq N_0$, we have that:
    \begin{align*}
        \abs{S_n(x) - S_m(x)} = \abs{\sum_{i=m+1}^n f_i(x)} \leq \sum_{i=m+1}^n \abs{f_i(x)} \leq \sum_{i=m+1}^n M_i
    \end{align*}
    Let $\e > 0$. Choose $N \geq N_0$ such that $\sum_{i = N+1}^\infty M_i < \e$ (which we can choose as the series converges by assumption). We then have that $\abs{S_n(x) - S_m(x)} < \e$ for all $n > m \geq N$ for all $x \in E$. Hence, $S_n(x)$ converges uniformly on $E$. \qed
\end{nproof}

\begin{theorem}{}{7.11}
    Let $E \subset X$ and $f_n: E \mapsto \RR \text{ or } \CC$, $n \in \NN$. Suppose $f_n \rightarrow f$ uniformly on $E$, and let $x \in E$ (where $x$ is a limit point of $E$). Suppose $\lim_{t \rightarrow x} f_n(t) = A_n$ exists for each $n \in \NN$. Then, $A_n \rightarrow A$ for some $A$ And $\lim_{t \rightarrow x} f(t) = A$. In other words:
    \begin{align*}
        \lim_{t \rightarrow x}\linf f_n(t) = \linf \lim_{t \rightarrow x} f_n(t)
    \end{align*}
    showing that the interchange of limits is valid when we have uniform convergence.
\end{theorem}
\begin{figure}[htbp]
    \centering
    \begin{tikzpicture}[scale=2]
        \draw[-latex, very thick] (0, 0) -- (2, 0);
        \draw[-latex, very thick] (0, 0) -- (0, 2);
        \draw[red] (0, 1) to [ curve through ={(0.5, 1.2)..(1,0.7)..(1.5,1.2)}] (1.7, 1.1);
        \draw[red, dotted, yshift = 5pt] (0, 1) to [ curve through ={(0.5, 1.2)..(1,0.7)..(1.5,1.2)}] (1.7, 1.1);
        \draw[red, dotted, yshift = -5pt] (0, 1) to [ curve through ={(0.5, 1.2)..(1,0.7)..(1.5,1.2)}] (1.7, 1.1);
        \draw[blue] (0, 1.15) to [curve through = {(0.5, 1.1)..(1, 0.8)..(1.5,1.1)}] (1.7, 1);
        \draw[blue, fill = white] (0, 1.15) circle (1pt);
        \node[left, text = blue] at (0, 1.15) {$A_n$};
        \draw[red, fill = white] (0, 1) circle (1pt);
        \node[left, text = red] at (0, 1) {$A$};
        \node[right, text = red] at (1.7, 1.15) {$f$};
        \node[right, text = blue] at (1.7, 0.95) {$f_n$};
    \end{tikzpicture}
    
    \caption{Visualization of Theorem \ref{thm:7.11}, with $E = (0, \infty)$ and $x = 0$. Eventually, the graph of $f_n$ lies in the $\e$ tube around $f$ (no matter how skinny the tube is). But, $A_n$ is being determined by $f_n$ near $0$, so there is nowhere for $A_n$ to go except to the limiting value. That is, $A_n \rightarrow A$ as the $\e$ tube gets compressed.}
    \label{fig42}
\end{figure}

\begin{nproof}
    We first show that $A_n \rightarrow A$ for some $A$. Since $\RR, \CC$ are complete metric spaces, it suffices to show that $\set{A_n}$ is Cauchy. Given $\e > 0$, choose $N$ such that for $m, n \geq N$, $\abs{f_n(t) - f_m(t)} < \e$ for all $t$ (such an $N$ exists by Theorem \ref{thm:7.8}). Letting $t \rightarrow x$, we therefore obtain that $\abs{A_n - A_m} \leq \e$ for all $m, n \geq N$, showing that $\set{A_n}$ is Cauchy. Hence, the sequence converges to some limit $A$. 

    Now, we show that $\lim_{t \rightarrow x}f(t) = A$. We show this by the common ``$\e/3$ argument''. For all $t \in E$ and $n \in \NN$, we have by the triangle inequality that:
    \begin{align*}
        \abs{f(t) - A} \leq \abs{f(t) - f_n(t)} + \abs{f_n(t) - A_n} + \abs{A_n - A} (*)
    \end{align*} 
    Which is a good move, as we know that we can make each of the three terms on the RHS arbitrarily small (they are ``close''). Let $\e > 0$. Since $f_n \rightarrow f$ uniformly, there exists $N_1$ such that $\abs{f(t) - f_n(t)} < \frac{\e}{3}$ for all $n \geq N_1$ and all $t \in E$. Since $A_n \rightarrow A$, there exists some $N_2$ such that $\abs{A_n - A} < \frac{\e}{3}$ for all $n \geq N_2$. Letting $N = \max\set{N_1, N_2}$ and taking $n = N$ in $(*)$, we have that:
    \begin{align*}
        \abs{f(t) - A} < \frac{\e}{3} + \abs{f_N(t) - A_N} + \frac{\e}{3} 
    \end{align*}
    Since $\lim_{t \rightarrow x} f_N(t) = A_N$, we can choose $\delta > 0$ such that $t \in N_{\delta}(x)$ implies $\abs{f_N(t) - A_N} < \frac{\e}{3}$ (Note a subtle point here that this choice of $\delta$ depends on $N$!). Therefore, if $t \in N_{\delta}(x)$, we have that:
    \begin{align*}
        \abs{f(t) - A} < \frac{\e}{3} + \frac{\e}{3} + \frac{\e}{3} = \e
    \end{align*}
    Hence, as $t \rightarrow x$, $f(t) \rightarrow A$. \qed
\end{nproof}

\begin{theorem}{}{7.12}
    Suppose $f_n$ is continuous on $E$ for all $n \in \NN$, and $f_n \rightarrow f$ uniformly on $E$. Then, $f$ is continuous.
\end{theorem}
\begin{nproof}
    Every $f_n$ is continuous at isolated points of $E$, so it suffices to consider limit points $x \in E' \cap E$. For these points, we have that:
    \begin{align*}
        f(x) = \lim_{n \rightarrow \infty}f_n(x) = \linf\lim_{t \rightarrow x} f_n(t) = \lim_{t \rightarrow x} \linf f_n(t) = \lim_{t \rightarrow x} f(t)
    \end{align*}
    Where the third equality (the interchange of the two limits) follows from Theorem \ref{thm:7.11}. We conclude that $f$ is continuous by Theorem \ref{thm:4.6} (as $f(x) = \lim_{t \rightarrow x} f(t)$). \qed
\end{nproof}

\begin{theorem}{}{7.13}
    Suppose $K$ is compact, and:
    \begin{enumerate}
        \item $f_n$ is continuous on $K$ for each $n \in \NN$
        \item $f_n \rightarrow f$ pointwise (that is, for each $x \in K$, $f_n(x) \rightarrow f(x)$) and $f$ is continuous
        \item $f_n(x) \geq f_{n+1}(x)$ for all $x \in K$ and all $n \in \NN$ (note that the opposite inequality also works, just multiply by $-1$).
    \end{enumerate}
    Then, $f_n \rightarrow f$ uniformly on $K$. 
\end{theorem}
\noindent Note that this theorem is not super useful, being that it requires so many specific assumptions; however, we will find that it does have an interesting proof. Before we move to that, let us show some counterexamples for when the assumptions do not hold.

\begin{nexample}{}{}
    Let $K = [-1, 0)$, and define:
    \begin{align*}
        f_n(x) = \begin{cases}
            0 & -1 \leq x \leq -\frac{1}{n}
            \\ 1 + nx & -\frac{1}{n} < x < 0
        \end{cases}
    \end{align*}
    We then have that $f_n$ is continuous on $K$, that $f_n \rightarrow 0$ pointwise on $K$, that $f$ is continuous (the zero function), and $f_n$ is decreasing with $n$. However, we note that $f_n$ does not converge uniformly to $f$ on $K$, with points close to zero being problem points (for example, take $x = -\frac{1}{2n}$, and then $f_n(x) - f(x) = \frac{1}{2}$ for all $n$). We note that $K$ is \textit{not} compact, showing the importance of compactness of the domain in the above Theorem.
\end{nexample}
\begin{figure}[htbp]
    \centering
    \begin{tikzpicture}[scale=1.5]
        \draw[latex-latex, very thick] (-2, 0) -- (2, 0);
        \draw[-latex , very thick] (0, 0) -- (0, 2);
        \draw[thick, blue] (-1.5, 0) -- (-0.5, 0);
        \draw[thick, blue] (-0.5, 0) -- (0, 1);
        \draw[blue, fill = white] (0, 1) circle (1pt);
        \draw[] (-1.5, 0) -- (-1.5, -0.15);
        \draw[blue, fill = blue] (-1.5, 0) circle (1pt);
        \draw[] (-0.5, 0) -- (-0.5, -0.15);
        \node[below] at (-0.5, -0.15) {$-\frac{1}{n}$};
        \node[below] at (-1.5, -0.15) {$-1$};
        \node[right] at (0, 1) {$1$};
    \end{tikzpicture}
    
    \caption{Plot of $f_n$ on $K = [-1, 0)$ from the above example.}
    \label{fig43}
\end{figure}

\begin{nexample}{}{}
    Let $K = [0, 1]$, and define:
    \begin{align*}
        f_n(x) = \begin{cases}
            nx & 0 \leq x \leq \frac{1}{n}
            \\ 2 - nx & \frac{1}{n} \leq x \leq \frac{2}{n}
            \\ 0 & \frac{2}{n} < x \leq 1
        \end{cases}
    \end{align*}
    We then have that $f_n$ is continuous, $f_n \rightarrow f = 0$ pointwise (which is continuous), and $K$ is compact. However, $f_n$ does not converge to $f$ uniformly. In this case, condition (c) of the above Theorem fails; $f_n$ is not monotonic in $n$.
\end{nexample}
\begin{figure}[htbp]
    \centering
    \begin{tikzpicture}[scale=2]
        \draw[-latex, very thick] (0, 0) -- (2, 0);
        \draw[-latex, very thick] (0, 0) -- (0, 2);
        \draw[] (1, 0) -- (1, -0.15);
        \draw[blue, thick] (0, 0) -- (0.5, 1);
        \draw[blue, thick] (0.5, 1) -- (1, 0);
        \draw[blue, thick] (1, 0) -- (1.5, 0);
        \draw[] (1.5, 0) -- (1.5, -0.15);
        \draw[blue, fill = blue] (1.5, 0) circle (1pt);
        \draw[blue, fill = blue] (0, 0) circle (1pt);
        \draw[] (0, 1) -- (-0.15, 1);
        \node[left] at (-0.15, 1) {$1$};
        \node[below] at (1.5, -0.15) {$1$};
        \draw[] (0.5, 0) -- (0.5, -0.15);
        \node[below] at (0.5, -0.15) {$\frac{1}{n}$};

        \node[below] at (1, -0.15) {$\frac{2}{n}$};

    \end{tikzpicture}
    
    \caption{Plot of $f_n$ on $K = [0, 1]$ from the above example.}
    \label{fig44}
\end{figure}

\begin{nproof}
    Let $g_n = f_n - f$. We can then see that:
    \begin{enumerate}
        \item $g_n$ is continuous (the difference of two continuous functions is continuous by Theorem \ref{thm:4.9})
        \item $g_n \rightarrow 0$ pointwise for all $x \in K$
        \item $g_n \geq g_{n+1} \geq 0$ for all $x \in K$.
    \end{enumerate}
    The goal will be to show that $g_n \rightarrow 0$ uniformly on $K$. We will use the finite intersection property of compact sets to show this. Let $\e > 0$. We will show that there exists $N$ such that $0 \leq g_n(x) < \e$ for all $n \geq N$ and for all $x \in K$. Note that it suffices to show that $g_N(x) < \e$ for some $N$, as $g$ is monotone decreasing in $n$. Define $K_n = g_{n}^{-1}([\e, \infty))$ (i.e. the set of ``bad $x$s''). We are done if we are able to show that there exists a $N$ with $K_N = \emptyset$. Since $g_n$ is conitnuous, $K_n$ is closed as $[\e, \infty)$ is closed. Since $K_n \subset K$, $K$ is therefore compact as a closed subset of a compact set (Theorem \ref{thm:2.35}). Additionally, we have that $K_{n+1} \subset K_n$, as $g_{n+1} \geq \e$ implies that $g_n \geq \e$. Since $g_n \rightarrow 0$ pointwise, given $x \in K$, there exsits $N_x$ such that $x \notin K_n$ for all $n \geq N_x$ (as $g_n(x) < \e$ for large enough $n$). We therefore have that $x \notin \bigcap_n K_n$ for all $x \in K$. Then, applying the corollary to Theorem \ref{thm:2.36}, we obtain that $K_N$ is empty. This means that for this $N$, $g_N^{-1}([\e, \infty)) = \emptyset$, and hence $g_N^{-1}([0, \e]) = K$, which is to say that $0 \leq g_n(x) < \e$ for all $x \in K$. \qed
\end{nproof}

\begin{definition}{$\C(X)$ and the Supremum Norm}{7.14}
    For a metric space $X$, define:
    \begin{align*}
        \C(X) = \set{f: X \mapsto \CC \text{ such that $f$ is bounded and continuous.}}
    \end{align*}
    The \textbf{supremum norm} of $f \in \C(X)$ is then defined as $\norm{f} = \sup_{x \in X}\abs{f(x)}$. We claim that $\norm{f - g}$ defines a metric on $\C(X)$, and we prove this assertion below. Thus, we have that:
    \begin{align*}
        f_n \rightarrow f \text{ uniformly} &\iff \forall \e > 0, \exists N \text{ such that } \abs{f_n(x) - f(x)} < \e \; \forall n \geq N \text{ and } \forall x \in X
        \\ &\iff \forall \e > 0, \exists N \text{ such that } \norm{f_n(x) - f(x)} < \e \; \forall n \geq N
        \\ &\iff f_n \rightarrow f \text{ in the metric space $\C(X)$}
    \end{align*}
    We have hence ``metrized'' uniform convergence.
\end{definition}
\begin{ntheorem}{}{}
    $\norm{f - g}$ defines a metric on $\C(X)$.
\end{ntheorem}
\begin{nproof}
    We recall the three properties of a metric as per Definition \ref{def:2.15}:
    \begin{enumerate}
        \item $d(f, g) = 0 \iff f = g$
        \item $d(f, g) = d(g, f)$
        \item $d(f, g) \leq d(f, h) + d(h, g)$
    \end{enumerate}
    We now show that $\norm{f - g}$ satisfies these three properties.
    \begin{enumerate}
        \item $\norm{f - g} = 0$ means that $0 = \sup_{x \in X}\abs{f(x) - g(x)} \implies \abs{f(x) - g(x)} = 0$ for all $x$, hence $f(x) = g(x)$. 
        \item $\norm{f - g} = \sup_{x \in X}\abs{f(x) - g(x)} = \sup_{x \in X}\abs{g(x) - f(x)} = \norm{g - f}$
        \item We have that $\abs{f(x) - g(x)} \leq \abs{f(x) - h(x)} + \abs{h(x) - g(x)}$ for all $x \in X$, so $\norm{f - g} \leq \norm{f - h} + \norm{h - g}$. \qed
    \end{enumerate}
\end{nproof}

\noindent Note that sometimes $\norm{f}$ is written as $\norm{f}_\infty$ as it is the $n \rightarrow \infty$ limit of the $L_p$ norm. See HW3Q3 for the proof that the supremum norm is the limit of the $L_p$ norm. 

\begin{theorem}{}{7.15}
    $\C(X)$ is a complete metric space (every Cauchy sequence in $\C(X)$ has a limit in $\C(X)$).
\end{theorem}
\begin{nproof}
    Let $\set{f_n}$ be a Cauchy sequence in $\C(X)$. Then, given $\e > 0$, $\exists N$ such that $m, n \geq N$ implies $\norm{f_m - f_n} = \sup_{x \in X}\abs{f_m(x) - f_n(x)} < \e$. By the Cauchy criterion (Theorem \ref{thm:7.8}), $f_n \rightarrow f$ for some $f$. What is left to show is that $f \in \C(X)$. $f$ is continuous as it is the uniform limit of continuous functions (Theorem \ref{thm:7.12}). Additionally, $f$ is bounded as there exists $N_0$ such that $\abs{f(x) - f_{N_{0}}(x)} < 1$ for all $x$, and hence $\abs{f(x)} \leq \abs{f_{N_0}(x)} + \abs{f(x) - f_{N_0}(x)} \leq M_0 + 1$ for all $x$ where $M_0$ is the bound on $f_{N_0}(x)$ that exists as $f_{N_0} \in \C(X)$. As $f$ is continuous and bounded, we conclude that $f \in \C(X)$. \qed
\end{nproof}

\subsection{Uniform Convergence and Integration}
\begin{theorem}{}{7.16}
    Suppose $f_n \in \R_{\alpha}[a, b]$ for all $n \in \NN$ and that $f_n \rightarrow f$ uniformly on $[a, b]$. Then, $f \in \R_{\alpha}[a, b]$, and:
    \begin{align*}
        \linf \int_{a}^b f_n d\alpha = \int_a^b f d\alpha
    \end{align*}
\end{theorem}

\newpage
\noindent In other words, the above Theorem tells us that we can interchange the integral with the limit if the sequence is uniformly convergent. Compare this to our earlier example with pointwise convergence, where such an interchange was not possible (as it yielded different values).

\begin{nproof}
    First, we show that $f \in \R_{\alpha}[a, b]$. Let $\e > 0$. Since $f_n \rightarrow f$ uniformly, there exists $N$ such that $\abs{f_n(X) - f(x)} < \e$ if $n \geq N$ for all $x \in [a, b]$. So, $f_n(x) - \e < f(x) < f_n(x) + \e$. Hence,
    \begin{align*}
        \lint{a}{b}(f_n - \e)d\alpha \leq \lint{a}{b}fd\alpha \leq \uint{a}{b} fd\alpha \leq \uint{a}{b}(f_n + \e)d\alpha 
    \end{align*}
    Since $f_n \pm \e \in \R_{\alpha}[a, b]$, we have that:
    \begin{align*}
        \int_{a}^b (f_n - \e)d\alpha \leq \lint{a}{b}fd\alpha \leq \uint{a}{b} fd\alpha \leq \int_a^b(f_n + \e)d\alpha 
    \end{align*}
    Therefore:
    \begin{align*}
        0 \leq \uint{a}{b} fd\alpha - \lint{a}{b} fd\alpha \leq \int_a^b 2\e d\alpha \implies \uint{a}{b} fd\alpha - \lint{a}{b} fd\alpha \leq 2\e(\alpha(b) - \alpha(a))
    \end{align*}
    Since $\e$ is arbitrary, we have that $\uint{a}{b} fd\alpha = \lint{a}{b} fd\alpha$ and hence $f \in \R_\alpha[a, b]$.

    Next, we show that $\linf \int_{a}^b f_n d\alpha = \int_a^b f d\alpha$. To do this, we show that $\abs{\int_a^b fd\alpha - \int_a^bf_nd\alpha}$ goes to 0 as $n \rightarrow \infty$. We have that:
    \begin{align*}
        \abs{\int_a^b fd\alpha - \int_a^bf_nd\alpha} = \abs{\int_a^b (f - f_n)d\alpha} \leq \int_a^b\abs{f - f_n}d\alpha \leq \int_a^b \e d\alpha = \e(\alpha(b) - \alpha(a))
    \end{align*}
    Where in the first equality we use Linearity (Theorem \ref{thm:6.12}), the first inequality we apply Theorem \ref{thm:6.13}, and in the second inequality, we use that for any $\e > 0$, there exists $N$ such that $\abs{f - f_n} < \e$ for $n \geq N$. Since $\e$ is arbitrary, we conclude that $\linf \int_{a}^b f_n d\alpha = \int_a^b f d\alpha$. \qed
\end{nproof}

\begin{ncorollary}{}{}
    If $f_n \in \R_\alpha[a, b]$ and $f(x) = \sum_{n=1}^\infty f_n(x)$ converges uniformly on $[a, b]$, then $\int_a^b fd\alpha = \sum_{n=1}^\infty \int_a^b f_n d\alpha$. That is to say, the infinite series and the integral can be interchanged.
\end{ncorollary}

\begin{nproof}
    Let $S_n(x) = \sum_{i=1}^n f_i(x)$. Then, $S_n(x) \rightarrow f(x)$ uniformly by assumption, so:
    \begin{align*}
        \int_a^b fd\alpha = \linf \int_a^b s_n d\alpha = \linf \sum_{i=1}^n \int_a^b f_i d\alpha = \sum_{i=1}^\infty \int_a^b f_i d\alpha
    \end{align*}
    Where the first equality follows from the previous theorem, and the second equality follows from the fact that a finite sum and integral can be interchanged by Linearity. \qed
\end{nproof}

\subsection{Uniform Convergence and Differentiation}
Recall Example \ref{exam:7.5}, where we looked at the sequence of functions $f_n(x) = \frac{\sin n x}{\sqrt{n}}$. We showed that $f_n \rightarrow 0$ uniformly on $\RR$, but we found in the example that $f_n'(x)$ does \emph{not} converge. We are therefore motivated to find a condition that if a function converges and is differentiable, then $f_n'$ converges.

As a point of notation, note that for $a < b$ we denote $\int_b^a f d\alpha = -\int_a^b fd\alpha$. 

\begin{theorem}{}{7.17}
    Suppose:
    \begin{enumerate}
        \item $f_n$ is differentiable on $[a, b]$;
        \item $\exists x_0 \in [a, b]$ such that $f_n(x_0)$ converges as $n \rightarrow \infty$;
        \item $f_n'$ converges uniformly on $[a, b]$. 
    \end{enumerate}
    Then, there exists $f$ such that $f_n \rightarrow f$ uniformly on $[a, b]$, and:
    \begin{align*}
        \linf f_n'(x) = f'(x) \; \forall x \in [a, b]
    \end{align*}
\end{theorem}
\noindent A couple remarks before we move to the proof. First, we note that hypothesis (b) seems strange; why would we require convergence $f_n$ at a single point? This has to do with the fact that in differentiating, we lost our constants. For example, let $f_n(x) = n$ as the simplest example. In this case, we have that $f_n$ is differentiable everywhere (with derivative zero everywhere on $[a, b]$) and that $gfn'$ uniformly converges (it is just the sequence of the zero function). However, $f_n$ does not even converge!

Note that we can and will assume that $f_n(x_0) \rightarrow 0$ at the specified $x_0$; if this is not true, we can simply replace $f_n(x)$ by $f_n(x) - f_n(x_0)$.

\begin{nproof}
    The proof of the above theorem is not so trivial. We will therefore prove a weaker theorem. Namely, we add a fourth hypothesis (d) that $f_n'$ is continuous on $[a, b]$. The proof of the stronger/original theorem can be found in Rudin.
    
    First, by (c) there exists a $g$ such that $f_n' \rightarrow g$ uniformly on $[a, b]$ (and also on any subinterval of $[a, b]$). Furthermore, by (d) and Theorem \ref{thm:7.12}, $g$ is continuous. 
    
    Next, applying Theorem \ref{thm:7.16} (to either $[x_0, x]$ or $[x, x_0]$) we have that:
    \begin{align*}
        \int_{x_0}^x f_n'(t) \rightarrow \int_{x_0}^x g(t) dt = f(x)
    \end{align*}
    Then applying the Fundamental theorem of calculus (Theorem \ref{thm:6.21}), we have $f_n(x) - f_n(x_0) \rightarrow f(x) \text{ and } f'(x) = g(x)$. But we also assume that $f_n(x_0) \rightarrow 0$, so $f_n(x) \rightarrow f(x)$ and $f_n'(x) \rightarrow g(x) = f'(x)$. So, we have shown pointwise convergence of $f_n$ to $f$! We have obtained that $\linf f_n'(x) = f'(x)$ for all $x \in [a, b]$. Finally, we show $f_n \rightarrow f$ uniformly on $[a, b]$. We have that:
    \begin{align*}
        \abs{f(x) - f_n(x)} = \abs{\int_{x_0}^x g(t)dt - \int_{x_0}^x f_n'(t) dt + f_n(x_0)} &\leq \int_{x_0}^x \abs{g(t) - f_n'(t)}dt + \abs{f_n(x_0)}
        \\ &< \int_{x_0}^x \frac{\e}{2(b-a)}dt + \frac{\e}{2} \leq \frac{\e}{2(b-a)}(b-a) + \frac{\e}{2} = \e
    \end{align*}
    Where we apply Theorem \ref{thm:6.13} for the first inequality, the fact that $f_n'(t) \rightarrow g$ and $f_n(x_0) \rightarrow 0$ in the second last inequality, and Theorem \ref{thm:6.12}(d) in the last inequality. \qed
\end{nproof}

\begin{theorem}{}{7.18}
    There exists a continuous function $f: \RR \mapsto \RR$ such that $f'(x)$ does not exist for any $x \in \RR$.
\end{theorem}

\noindent The proof of the above theorem will follow by the construction of an ``infinitely spiky'' real function. Though this might seem like a very pathological counterexample, there are actually many examples of non-differentiable phenomena in mathematics. Looking at the field of probability, we find that brownian motion, brownian maps, and discrete exploration processes (to name a few) all have this property. A visualization of the brownian map, as well as other beautiful probability pictures can be found here \url{https://secure.math.ubc.ca/Links/Probability/pages/pic_gallery.html}.

\begin{nproof}
    Define $\phi: \RR \mapsto \RR$ by $\phi(x) = \abs{x}$ for $-1 \leq x \leq 1$ and $\phi(x + 2) = \phi(x)$ for all $x \in \RR$. (See figure \textbf{REF}). Then, $\phi$ is continuous; moreover, it is Lipschiz continuous, with $\abs{\phi(s) - \phi(t)} \leq \abs{s - t}$ for all $s, t \in \RR$ (with equality where there are no integers between $s, t$). Define $f(x) = \sum_{n=0}^\infty \left(\frac{3}{4}\right)^n \phi(4^nx)$. The series converges uniformly on $\RR$ by Theorem \ref{thm:7.10}, since $0 \leq \left(\frac{3}{4}\right)^n \phi(4^nx) \left(\frac{3}{4}\right)^n$ and $\sum_n\left(\frac{3}{4}\right)^n$ converges (it is a geometric series with $r < 1$). Hence, $f$ is continuous as it is a uniform limit of a continuous function (Theorem \ref{thm:7.12}). We now prove that $f'(x)$ does not exists for any $x \in \RR$; let us then fix $x$. It suffices to find $\delta_m \rightarrow 0$ such that:
    \begin{align*}
        \abs{\frac{f(x + \delta_m) - f(x)}{\delta_m}} \rightarrow \infty \text{ as } m \rightarrow \infty.
    \end{align*}
    We then choose $\delta_m = \pm \frac{1}{2}\frac{1}{4^m}$. We choose the sign of $\delta_m$ depending on the choice of $x$ as follows. At most one of $(4^m x - \frac{1}{2}, 4^mx)$ and $(4^mx, 4^mx + \frac{1}{2})$ contains an integer (See figure \textbf{REF}). We choose the sign such that no integer lies between $4^m x$ and $4^m(x + \delta_m)$. Note that we may choose a differnet sign for each $m$. 
    
    Next, we make the observation that $\abs{\phi(4^m(x + \delta_m)) - \phi(4^mx)} = 4^mx$; this holds as for the difference between two $\phi(x)$ values at two points without an integer between them is just the difference between the $x$ values. Looking back at our definition of $\delta_m$, we then see that $\abs{\phi(4^m(x + \delta_m)) - \phi(4^mx)} = \frac{1}{2}$.
\end{nproof}

\subsection{Equicontinuituous Families of Functions}

\subsection{The Stone-Weierstrauss Theorem}

