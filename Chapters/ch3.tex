\newpage
\section[Numerical Sequences and Series]{\hyperlink{toc}{Numerical Sequences and Series}}
\subsection{Sequences}
We begin by formally defining a sequence.
\begin{ndef}{: Sequences}{}
    Let $X$ be a metric space. A \textbf{sequence} is a function $f: \NN \mapsto X$. We can denote a term in the sequence as $f(n) = x_n$, or the entire sequence as $\set{x_n}_{n=1}^\infty$, $\set{x_n}$, $(x_n)$, or $\set{x_1, x_2, x_3 \ldots}$. 
\end{ndef}
\noindent We now discuss the notion of convergence of a sequence. Intuitively, we can equate convergence with the notion of points getting closer together.
\begin{definition}{Convergence of Sequences}{3.1}
    A sequence $\set{p_n}_{n=1}^\infty$ \textbf{converges} to $p \in X$ if for all $\e > 0$, there exists $N \in \NN$ such that $n \geq N$ implies $d(p_n, p) < \e$. In this case, we say that $\set{p_n}$ converges to $p$, or that $p$ is the limit of $\set{p_n}$, and denote this as $p_n \rightarrow p$ or $\lim_{n \rightarrow \infty} p_n = p$. If $\set{p_n}$ does not converge, we say it \textbf{diverges}.
\end{definition}
\noindent To phrase this definition in another way, we fix some $\e > 0$, and then we have that all points in the sequence past some $N \in \NN$ are contained in the neighbourhood $N_\e(p)$. In practice, it can be difficult to apply this definition of convergence if we don't know what the limiting $p$ is, as the definition implicitly uses the value of the limit. We will later discuss another definition of convergence (in $\RR^k$) that does not use the value of the limit.
\begin{figure}[htbp]
    \centering
    \begin{tikzpicture}[mycirc/.style={circle,fill, minimum size=0.15cm, inner sep = 0pt}]
        \draw[red, dotted, thick, fill = white!60!red] (-1, 0) circle (30pt);
        \node[mycirc, label=above:{$p$}, fill = red] at (-1, 0) {};
        \node[mycirc, label=above:{$p_1$}] at (1.5, 0) {};
        \node[mycirc, label=above:{$p_2$}] at (1, -0.25) {};
        \node[mycirc, label=above:{$p_3$}] at (0.5, -0.5) {};
        \node[mycirc, label=above:{$p_4$}] at (0, -0.5) {};
        \node[mycirc, label=above:{$p_5$}] at (-0.35, -0.4) {};
        \node[mycirc] at (-0.65, -0.2) {};
        \node[mycirc] at (-0.75, -0.15) {};
        \node[mycirc] at (-0.85, -0.1) {};
        \node[mycirc] at (-0.95, -0.05) {};
        \draw[<->] (-1.08, 0) -- (-2.05, 0);
        \node[label=above:{$\e$}] at (-1.5, -0.15) {};
    \end{tikzpicture}
    \caption{Visualization of a sequence $\set{p_n} \subset \RR^2$ converging to a point $p$. For the $\e > 0$ shown in the picture, we have that all points of the sequence past $N = 5$ lie in the open disk of radius $\e$ around $p$.}
    \label{fig15}
\end{figure}

\noindent As a remark, consider that convergence can depend on our choice of metric space; for example, $\set{\frac{1}{n}}$ as a sequence in $\RR$ converges to $0$, but the same sequence in the strictly positive reals ($\RR^+ = \set{x \in \RR: x > 0}$) does not converge.

Another interesting example (that again shows us the importance of the choice of metric space). Is $\RR$ equipped with the discrete metric. A question we can ask is ``given some points $p \in \RR$, what sequences converge to $p$?'' The answer turns out to be eventually constant sequences only; that is, sequences for which $p_n = p$ for $n \geq N$ for some $N$. 
\begin{proof}
    If $p_n \rightarrow p$, then setting $\e = \frac{1}{2}$, we have that there exists $N \in NN$ such that for all $n \geq N$, $d(p_n, p) < \e = \frac{1}{2}$. Under the discrete metric, this is only possible if $p_n = p$. 
\end{proof}
This of course is a strikingly different picture for $\RR$ with the standard metric of $d(x, y) = \abs{x - y}$. For example, the sequnce $p_n = \frac{1}{n}$ has no term equal to zero, but converges to $p = 0$. The takeaway message here can be that in the Euclidean metric, points can ``get closer'' but in the discrete metric, they cannot.

\stepcounter{rudin}
\begin{theorem}{}{3.3}
    Suppose $\set{s_n}, \set{t_n}$ are complex sequences that converge, with $\linf s_n \rightarrow s$ and $\linf t_n \rightarrow t$. Then:
    \begin{enumerate}
        \item $\linf(s_n + t_n) = s + t$.
        \item $\linf cs_n = cs$ and $\linf (c + s_n) = c + s$ for all $c \in \CC$.
        \item $\linf s_nt_n = st$
        \item $\linf \frac{1}{s_n} = \frac{1}{s}$ provided $s \neq 0$ and $s_n \neq 0$ for all $n$.
    \end{enumerate}
\end{theorem}
\begin{nproof}
    \begin{enumerate}
        \item Let $\e > 0$. There exist $N_1, N_2 \in \NN$ such that $\abs{s_{n_1} - s} < \frac{\e}{2}$ for $n_1 \geq N_1$ and $\abs{t_{n_2} - t} < \frac{\e}{2}$ for $n_2 \geq N_2$. Take $N = \max{N_1, N_2}$, and using the triangle inequality, it follows that for $n \geq N$:
        \begin{align*}
            \abs{(s_n + t_n) - (s + t)} \leq \abs{s_n - s} + \abs{t_n - t} < \frac{\e}{2} + \frac{\e}{2} = \e
        \end{align*} 
        We conclude that $\linf (s_n + t_n) = s + t$. 
        \item Let $\e > 0$. If $c = 0$ then the first sequence trivially converges to $0$, so suppose that $c \neq 0$. There exists $N$ such that $\abs{s_n - s} < \frac{\e}{\abs{c}}$ for $n \geq N$, so it follows that:
        \begin{align*}
            \abs{cs_n - cs} = \abs{c}\abs{s_n - s} < \abs{c}\frac{\e}{\abs{c}} = \e.
        \end{align*} For the second identity, we have that $c_n \rightarrow c$ for any constant sequence $c_n = c$ so we may apply (a).
        \item Let $\e > 0$. There exist $N_1, N_2$ such that $\abs{s_{n_1} - s} < \sqrt{2}$ for $n_1 \geq N_1$ and $\abs{t_{n_2} - t} < \sqrt{2}$ for $n_2 \geq N_2$. We then consider that:
        \begin{align*}
        s_nt_n - st = (s_n - s)(t_n - t) + s(t_n - t) + t(s_n - s)
        \end{align*}
        For $n \geq N = \max{N_1, N_2}$, we have that:
        \begin{align*}
            (s_n - s)(t_n - t) < \e
        \end{align*}
        And we hence observe that $\linf (s_n - s)(t_n - t) = 0$. We can then use (a) and (b) to find that:
        \begin{align*}
            \linf s(t_n - t) = 0, \quad \linf t(s_n - s) = 0
        \end{align*}
        So we conclude that $\linf (s_nt_n - st) = 0$ and hence $s_nt_n \rightarrow st$.
    \end{enumerate}
\end{nproof}
\begin{nproofcont}
    \begin{enumerate}
        \setcounter{enumi}{3}
        \item Choose $m$ such that $\abs{s_n - s} < \frac{1}{2}\abs{s}$ if $n \geq m$. We then have that $\abs{s_n} > \frac{1}{2}\abs{s}$ for $n \geq m$. Let $\e > )0$. Then, there exists $N$ with $N > m$ such that for $n \geq N$:
        \begin{align*}
            \abs{s_n - s} < \frac{1}{2}\abs{s}^2\e
        \end{align*}
        Hence, for $n \geq N$:
        \begin{align*}
            \abs{\frac{1}{s_n} - \frac{1}{s}} = \abs{\frac{s_n - s}{s_ns}} < \frac{2}{\abs{s}^2}\abs{s_n - s} < \e
        \end{align*}
        \qed
    \end{enumerate}
\end{nproofcont}

\begin{nlemma}{: Squeeze Lemma}{}
    Let $\set{x_n}$, $\set{s_n}$ be real-valued sequences. Then, if $0 \leq x_n \leq s_n$ for all $n$, and $\linf s_n = 0$, then $\linf x_n = 0$.
\end{nlemma}
\begin{nproof}
    Let $\e > 0$. Choose $N \in \NN$ such that $n \geq N$ implies $0 \leq s_n < \e$. Then, we have that for $n \geq N$, $0 \leq x_n \leq s_n < \e$ and hence $x_n \rightarrow 0$ as claimed. \qed
\end{nproof}

\noindent Note that we can prove a more generalized version of the Squeeze Lemma. 

\begin{nlemma}{: Generalized Squeeze Lemma}{}
    Suppose we have sequences $\set{l_n}, \set{x_n}, \set{u_n}$ such that $l_n \leq x_n \leq u_n$ for all $n$ and $\linf l_n = \linf u_n = L \in \RR$. Then, $\linf x_n = L$.
\end{nlemma}

\begin{nproof}
    We have that $0 \leq a_n - l_n \leq u_n - l_n$. We have that $\linf u_n - l_n = 0$ by Theorem \ref{thm:3.3}(a), so by the (original) Squeeze Lemma we have that $\linf a_n - l_n = 0$. It then follows that $\linf a_n = \linf l_n = L$ as claimed. \qed
\end{nproof}

\setcounter{rudin}{19}
\begin{theorem}{}{3.20}
    \begin{enumerate}
        \item Let $p > 0$. Then, $\linf \frac{1}{n^p} = 0$.
        \item Let $p > 0$. Then, $\linf \sqrt[n]{p} = 1$.
        \item $\linf \sqrt[n]{n} = 1$.
        \item Let $p > 0$ and $\alpha \in \RR$. Then, $\linf \frac{n^\alpha}{(1+p)^n} = 0$.
        \item Let $\abs{x} < 1$. Then, $\linf x^n = 0$. 
    \end{enumerate}
\end{theorem}
\begin{nproof}
    \begin{enumerate}
        \item Let $\e > 0$. Choose $N$ such that $\frac{1}{N^p} < \e$, namely $N > \left(\frac{1}{\e}\right)^{1/p}$. Then, for $n \geq N$, $\frac{1}{n^p} < \frac{1}{N^p} < \e$. 
        
        \item If $p = 1$, the sequence is constant and the conclusion immediate. 
        
        If $p > 1$, then let $x_n = \sqrt[n]{p} - 1$. We then have that:
        \begin{align*}
            p = (x_n + 1)^n = \sum_{k=0}^n \binom{n}{k}x_n^k \geq nx_n
        \end{align*}
        Where the second equality follows from the binomial theorem (where $\binom{n}{k} = \frac{n!}{k!(n-k)!}$), and the inequality follows by considering that we just keep the $k = 1$ term (and the series is non-negative). Hence, we have that $x_n \leq \frac{p}{n}$, and $x_n \rightarrow 0$ by (a). 
        
        If $p < 1$, then let $q = \frac{1}{p} > 1$. Then, $\sqrt[n]{q} \rightarrow 1$ by the argument above. By Theorem \ref{thm:3.3}(d), we then have that $\sqrt[n]{p} = \frac{1}{\sqrt[n]{q}} \rightarrow \frac{1}{1} = 1$.

        \item Let $x_n = \sqrt[n]{n} - 1$. Then, we have that:
        \begin{align*}
            n = (x_n + 1)^n = \sum_{k=0}^n\binom{n}{k}x_n^k \geq \frac{n(n-1)}{2}x_n^2
        \end{align*}
        Where the inequality follows from keeping the $k = 2$ term only. We then have that $x_n \leq \sqrt{\frac{2}{n-1}}$ and hence $x_n \rightarrow 0$ by the Squeeze Lemma.
        \item We want to show $\frac{n^\alpha}{(1+p)^n} \rightarrow 0$; we therefore want an upper bound on the expression, and hence a lower bound on $(1+p)^n$. Applying the Binomial Theorem we have that:
        \begin{align*}
            (1+p)^n = \sum_{k=0}^n\binom{n}{k}p^k = \left((n)(n-1)(n-2)\cdots(n-k+1)\right)\frac{p^k}{n!}
        \end{align*}
        Now, we pick $k > \alpha$. For $2n > k$, we then have that:
        \begin{align*}
            (1+p)^n \geq \left(\frac{n}{2}\right)^k\frac{p^k}{k!}
        \end{align*}
        We therefore have that:
        \begin{align*}
            \frac{n^\alpha}{(1+p)^k} \leq \frac{2^kk!}{p^k}n^{\alpha - k} \rightarrow 0
        \end{align*}
        And the claim follows by the Squeeze Lemma.
        
        \item Taking $\alpha = 0$ in (d), the claim follows by setting $\abs{x} = \frac{1}{1+p} < 1$ (as $p > 0$) and recognizing that $x_n \rightarrow 0 \iff \abs{x^n} = \abs{x}^n \rightarrow 0$. \qed
    \end{enumerate}
\end{nproof}

\subsection{Subsequences}

\setcounter{rudin}{1}
\begin{theorem}{}{3.2}
    Let $\set{p_n}$ be a sequence in $X$.
    \begin{enumerate}
        \item $p_n \rightarrow p$ in $X$ if and only if for all $r > 0$, $N_r(p)$ contains all but finitely many points of $\set{p_n}$.
        \item If $p_n \rightarrow p$ and $p_n \rightarrow p'$ then $p = p'$. In other words, the limit is unique.
        \item If $\set{p_n}$ is convergent, then it is bounded (that is, for any $q \in X$ there exists $M \in \RR$ such that $d(q, p_n) \leq M$ for all $n \in \NN$).
        \item If $E \subset X$ has a limit point $p$, then there exists $\set{p_n}$ in $E$ such that $p_n \rightarrow p$. 
    \end{enumerate}
\end{theorem}
\begin{nproof}
    \begin{enumerate}
        \item The claim follows immediately from the definition of convergence; for any $r = \e > 0$, there exists $N \in \NN$ such that $N_r(p)$ contains $\set{p_n: n \geq N}$.
        \item There exist $N_1, N_2$ such that $d(p, p_{n_1}) < \frac{\e}{2}$ if $n_1 \geq N_1$ and $d(p, p_{n_2}) < \frac{\e}{2}$ if $n_2 \geq N_2$. Then for $n \geq N = \max{N_1, N_2}$ we have (using the triangle inequality) that:
        \begin{align*}
            d(p, p') \leq d(p, p_n) + d(p_n, p') < \frac{\e}{2} + \frac{\e}{2} = \e
        \end{align*}
        Since $\e$ is arbitrary, $d(p, p') = 0$ and hence $p = p'$.

        \item If $p_n \rightarrow p$, there exists $N$ such that $d(p_n, p) < 1$ for all $n \geq N$. Set:
        \begin{align*}
            r = \max\set{1, d(p_1, p), d(p_2, p), \ldots ,d(p_{N-1}, p)}
        \end{align*}
        For any $q \in X$, we then have that:
        \begin{align*}
            d(q, p_n) \leq d(q, p) + d(p, p_n) \leq d(q, p) + r
        \end{align*}
        so the claim follows with $M = r + d(q, p) + 1$.
        \item Pick $p_n \in E$ such that $d(p_n, p) < \frac{1}{n}$. Let $\e > 0$, and $N > \frac{1}{\e}$. Then, $n \geq N$ implies $\frac{1}{n} \leq \frac{1}{N} < \e$ and hence $d(p_n, p) < \e$ for all $n \geq N$, and hence $p_n \rightarrow p$ as desired. \qed
    \end{enumerate}
\end{nproof}

\setcounter{rudin}{4}
\begin{definition}{Subsequences}{3.5}
    Given $\set{p_n}$ and $n_1 < n_2 < n_3 < \ldots$, we say that $\set{p_{n_j}}$ is a \textbf{subsequence} of $\set{p_n}$.
\end{definition}

\noindent We first consider some examples. Let $p_n = n$. Then some valid subsequences of $\set{p_n}$ are $\set{1, 2, 3, 4, 5, \ldots}$ (the original sequence), $\set{1, 3, 5, 7, \ldots}$ (the odds), $\set{2, 3, 5, 7, 11, 13, \ldots}$ (the primes). Next, let $p_n = i^n$. We have that $\set{p_n} = \set{i, -1, -i, 1, i, -1, -i, 1, \ldots}$ which is clearly divergent. However, the subsequences $\set{i, i, i, \ldots}$, $\set{-1, -1, -1, \ldots}$, $\set{-i, -i, -i, \ldots}$ and $\set{1, 1, 1, \ldots}$ are all convergent! It is hence possible for a divergent sequence to have a convergent subsequence.

\begin{nlemma}{}{}
    If $p_n \rightarrow p$, then every subsequence of $\set{p_n}$ converges to $p$. 
\end{nlemma}

\begin{nproof}
    Suppose $p_n \rightarrow p$ and let $\set{p_{n_j}}$ be a subsequence of $p_n$. Let $\e > 0$. Then, there exists some $N \in \NN$ such that $d(p, p_n) < \e$ if $n \geq N$. Hence, $d(p, p_{n_j}) < \e$ if $n_j \geq N$ and hence $p_{n_j} \rightarrow p$. \qed
\end{nproof}

\begin{theorem}{Bolzano–Weierstrass}{3.6}
    \begin{enumerate}
        \item If $\set{p_n} \subset X$ with $X$ compact, then $\set{p_n}$ has a convergenct subsequence.
        \item If $\set{p_n} \subset \RR^k$ and $\set{p_n}$ is bounded, then $\set{p_n}$ has a convergent subsequence.
    \end{enumerate}
\end{theorem}
\begin{nproof}
    \begin{enumerate}
        \item Let $E$ be the range of $\set{p_n}$. If $E$ is finite, then there exists $x \in X$ and $n_1 < n_2 < n_3 < \ldots$ such that $p_{n_j} = x$ for all $j$. Therefore $p_{n_j} \rightarrow x$ and we are done. If $E$ is infinite, then by compactness, $E \subset X$ has a limit point in $X$ by Theorem \ref{thm:2.37}. By Theorem \ref{thm:3.2}(d) there exists a sequence $\set{p_{n_j}}$ in $E$ such that $p_{n_j} \rightarrow p$.
        \item By Theorem \ref{thm:2.41}, $E$ (being bounded) lies in a compact subset of $\RR^k$. We then apply (a). \qed
    \end{enumerate}
\end{nproof}

\subsection{Cauchy Sequences and Completeness}

\setcounter{rudin}{7}
\begin{definition}{Cauchy Sequences}{3.8}
    A sequence $\set{p_n} \subset X$ is a \textbf{Cauchy sequence} if for all $\e > 0$, there exists $N \in \NN$ such that for all $n, m \geq N$, $d(p_n, p_m) < \e$. 
\end{definition}
\noindent Note the fact that this definition does not refer to a particular $p$ that the sequence may converge to! It instead formalizes the notion of the points of a sequence getting ``closer together'' as the sequence goes on. It is therefore easier to check if a sequence is Cauchy than if it converges, as we don't need to know the value of the limit. To this end, it is useful to know in what situations a sequence being Cauchy implies that the sequence is convergent. We will soon arrive at a theorem that addresses this question, but first we establish a little more machinery.

\begin{definition}{Diameter}{3.9}
    Let $E \subset X$. Then the \textbf{diameter} of $E$, denoted $\diam E$ is defined as $\diam E = \sup\set{d(p, q): p, q \in E}$. It follows from the definition that a sequence $\set{p_n}$ is Cauchy if and only if $\lim_{N \rightarrow \infty} \diam E_n = 0$ where $E_n = \set{p_n}_{n=N}^\infty$ (the tail of the sequence).
\end{definition}

\begin{nexample}{}{}
    \begin{enumerate}
        \item If $E = (a, b) \subset \RR$ or $E = [a, b] \subset \RR$, then $\diam E = b - a$.
        \item If $E = (0, 1) \times (0,1) \subset \RR^2$, then $\diam E = \sqrt{2}$ (the diagonal of the open square).
    \end{enumerate}
\end{nexample}

\begin{theorem}{}{3.10}
    \begin{enumerate}
        \item Let $E \subset X$. Then, $\diam \overline{E} = \diam E$.
        \item If $K_n \subset X$ are compact, $K_{n+1} \subset K_n$ for all $n$, and $\linf \diam K_n  = 0$, then $\bigcap_{n=1}^\infty K_n$ consists of exactly one point.
    \end{enumerate}
\end{theorem}

\begin{nproof}
    \begin{enumerate}
        \item Since $E \subset \overline{E}$, it is clear that $\diam \overline{E} \geq \diam E$. Next, let $\e > 0$ and $p, q \in \overline{E}$. Choose $p', q' \in E$ such that $d(p, p') < \frac{\e}{2}$, $d(q, q') < \frac{\e}{2}$ (this choice is possible as either $p, q$ are in $E$, or $p, q$ are limit points of $E$). Then, we have that:
        \begin{align*}
            d(p, q) \leq d(p, p') + d(p', q) \leq d(p, p') + d(p', q') + d(q', q) < \frac{\e}{2} + \diam E + \frac{\e}{2} = \diam E + \e
        \end{align*}
        $\e, p$, and $q$ are arbitrary, so it follows that $\diam \overline{E} \leq \diam E + \e$ from the definition of the diameter. It then follows that $\diam \overline{E} \leq \diam E$. We conclude that $\diam \overline{E} = \diam E$.
        \item Let $K = \bigcap_{n=1}^\infty K_n$. By the corollary to Theorem \ref{thm:2.36}, we have that $K \neq \emptyset$, so $K$ contains at least one point. Since $K \subset K_n$, it follows that $\diam K \leq \diam K_n$ for any $n$, and since $\diam K_n \rightarrow 0$, $\diam K = 0$. If there were $p, q \in K$ such that $p \neq q$, then $\diam K \neq 0$, so it must follow that $K$ has at most one point. We conclude that $K$ has exactly one point. \qed
    \end{enumerate}
\end{nproof}

\begin{nlemma}{}{}
    If a sequence $\set{p_n}$ is Cauchy, then it is bounded.
\end{nlemma}
\begin{nproof}
    If $\set{p_n}$ is Cauchy, then we have that $\lim_{N \rightarrow \infty} \diam E_N = \lim_{N \rightarrow \infty} \diam \set{p_n}_{n = N}^\infty = 0$. Then for some $N \in \NN$, $\diam E_N < 1$. The range of $\set{p_n}$ is the union of $E_N$ and the finite set $\set{p_1, \ldots, p_{N-1}}$ and hence $\set{p_n}$ is bounded. \qed
\end{nproof}

\begin{theorem}{}{3.11}
    \begin{enumerate}
        \item If a sequence $\set{p_n} \subset X$ converges, then it is Cauchy.
        \item If a sequence $\set{p_n} \subset X$ is Cauchy and $X$ is compact, then $\set{p_n}$ converges to some $p \in X$.
        \item In $\RR^k$, every Cauchy sequence is convergent.
    \end{enumerate}
\end{theorem}

\begin{nproof}
    \begin{enumerate}
        \item Let $p_n \rightarrow p$ and let $\e > 0$. There exists $N \in \NN$ such that $d(p_n, p) < \frac{\e}{2}$ if $n \geq N$. Then, for $n, m \geq N$, we have that:
        \begin{align*}
            d(p_n, p_m) \leq d(p_n, p) + d(p, p_m) < \frac{\e}{2} + \frac{\e}{2} = \e
        \end{align*}
        so $\set{p_n}$ is Cauchy.
        \item Let $E_N = \set{p_n}_{n = N}^{\infty}$. Then, $\overline{E}_N \subset X$ is closed, so by the compactness of $X$ we have that $\overline{E}_N$ is compact by Theorem \ref{thm:2.35}. Since $E_{N+1} \subset E_N$, we have that $\overline{E}_{N+1} \subset \overline{E}_N$, and additionally we have that $\lim_{N \rightarrow \infty} \overline{E}_N =\lim_{N \rightarrow \infty} E_N = 0$ where the first equality follows from Theorem \ref{thm:3.10}(a) and the second equality follows from the fact that $\set{p_n}$ is Cauchy and Definition \ref{def:3.9}. Thus, Theorem \ref{thm:3.10}(b) says that there exists a unique point $p \in \bigcap_{n=1}^\infty \overline{E}_N$. Next, let $\e > 0$. Then, there exists $N_0$ such that $\diam \overline{E}_N < \e$ for all $N \geq N_0$. So, $d(p, q) < \e$ for all $q \in \overline{E}_N$, so the same holds for all $q \in E_N$. Hence, $d(p, p_n) < \e$ for all $n \geq N_0$, which shows that $p_n \rightarrow p$ and proves the claim. 
        \item By the above Lemma, Cauchy sequences are bounded. Hence, $\set{p_n} \subset I$ for some $k$-cell $I \subset \RR^k$. Since $I$ is compact in $\RR^k$, the claim follows from (b). \qed
    \end{enumerate}
\end{nproof}

\begin{definition}{Completeness}{3.12}
    A metric space $X$ is called \textbf{complete} if every Cauchy sequence converges in $X$. 
\end{definition}
It might be tempting at first to think that every space would be complete, but this is not the case. For example, something that can go wrong is a sequnece can be Cauchy, but the limit can lie ``outside'' of the space. To see this, consider again the sequence $\set{\frac{1}{n}}$ in the metric space $\RR^+ = \RR \setminus \set{x \in \RR: x \leq 0}$. The sequence is Cauchy, but does not converge in $\RR^+$ (as it converges to 0, which lies outside of the space).

\begin{nexample}{}{}
    \begin{enumerate}[(i)]
        \item Compact sets are complete by Theorem \ref{thm:3.11}(b).
        \item $\RR^k$ (and $\CC$) are complete by Theorem \ref{thm:3.11}(c).
        \item $\QQ$ is not complete. We can make a sequence of rational points that converges to an irrational number in $\RR$ which is Cauchy, but does not converge in $\QQ$ (Example \ref{exam:1.1b} gives a way one might construct such a sequence). 
    \end{enumerate}
\end{nexample}
\noindent Note that $\QQ$ can be completed to $\RR$, and in general for any $(X, d)$ which is not complete, there exists $(X^*, d^*)$ that is complete such that $\abs{X} = X^*$. Indeed, this is another way we can construct the real numbers! $\RR$ can be viewed as equivalence classes of Cauchy sequences in $\QQ$. The idea is to define an equivalnence relation $\sim$ such that $p_n \sim q_n$ if $\linf d(p_n, q_n) = 0$. $X^*$ is then defined as the set of equivalence classes under that equivalence relation, equipped with the metric $d^*([p], [q]) = \linf d(p_n, q_n)$. It can then be checked that $d^*$ is a valid metric and that $X^*$ is complete. For the full proof, see HW7, or exercises 3.23-3.25 in Rudin (note: this proof is quite technical/difficult).

To motivate the next theorem, consider that all convergent sequences in $\RR$ (and in general) are bounded (as we saw in Theorem \ref{thm:3.2}(c)). However, this is not always true; for example consider $p_n = (-1)^n$ which is clearly bounded but divergent. What then are conditions that a bounded sequence may converge?

\begin{definition}{Monotonic Sequences}{3.13}
    A sequence $\set{p_n} \subset \RR$ is \textbf{monotonically increasing} if $p_{n+1} \geq p_n$ for all $n$, and \textbf{montonically decreasing} if $p_{n+1} \leq p_n$.
\end{definition}

\begin{theorem}{}{3.14}
    Suppose $\set{p_n} \subset \RR$ is montonic. Then, $\set{p_n}$ is convergent if and only if it is bounded. 
\end{theorem}

\begin{nproof}
    $\boxed{\implies}$ See Theorem \ref{thm:3.2}(c).

    $\boxed{\impliedby}$ We show the proof for the increasing case as the decreasing case is analogous. Let $p = \sup{p_n: n \in \NN}$ which exists as $\set{p_n}$ is bounded and $\RR$ has the LUB property. Then, $p_n \leq p$ for all $n$. Let $\e > 0$. Then, there exists $N \in \NN$ such that $p - \e < p_{N} < p_{N+1}$. By the monotonicity of $\set{p_n}$, it follows that $\abs{p_n - p} < \e$ for all $n \geq N$. Hence, $p_n \rightarrow p$. \qed
\end{nproof}

\begin{definition}{Limits to Infinity}{3.15}
    Let $\set{p_n} \subset \RR$. If for all $M \in \RR$, there exists $N \in \NN$ such that $p_n > M$ for all $n \geq N$, then we write $p_n \rightarrow \infty$. If instead for all $M \in \RR$ there exists $N \in \NN$ such that $p_n < M$ for all $n \geq N$, then we write $p_n \rightarrow -\infty$.
\end{definition}



\subsection{Limit Supremum and Limit Infimum}
As a motivating question, how would we say something about the largest and smallest accumulation points of a sequence? This leads us to the following definition.
\begin{definition}{limsup and liminf}{3.16}
    Let $\set{s_n} \subset \RR$, then, we define the \textbf{limit supremum} as:
    \begin{align*}
        \limsup_{n \rightarrow \infty} s_n = \inf_{n \geq 1}\sup_{m \geq n} s_m = \lim_{n \rightarrow \infty} \sup_{m \geq n} s_m
    \end{align*}
    And the \textbf{limit infimum} as:
    \begin{align*}
        \liminf_{n \rightarrow \infty} s_n = \sup_{n \geq 1}\inf_{m \geq n} s_m = \lim_{n \rightarrow \infty} \inf_{m \geq n} s_m
    \end{align*}
    Note that unlike the limit of a real-valued sequence, the limsup and liminf always exist.
\end{definition}

\noindent In the above definition, the equivalence of $inf_{n \geq 1}\sup_{m \geq n} s_m$ and $\lim_{n \rightarrow \infty} \sup_{m \geq n} s_m$ may be slightly confusing. To see this, consider the fact that the sequence $q_n = \sup \set{p_n: n \geq 1}$ is a strictly decreasing sequence in $n$ (with increasing $n$, we take the supremum over less terms each time), so taking the limit of $n \rightarrow \infty$ or taking the infimum over $n$ are equivalent. 

Note that Rudin defines the limsup/liminf differently, but perfectly equivalently. Namely, if $\set{p_n} \subset \RR$ is a sequence, then $E$ is the set of all subsequential limits (i.e. there is a subsequence of $\set{p_n}$ with a given limit). Then, $\limsup^* p_n = \sup E$ (we use the $*$ to denote Rudin's definition). The equivalence is not immediately obvious, so we here give a sketch to show that the two definitions coincide. We will use the general technique of showing that the two expressions are $\e$ close to one another. Namely, for any $\e > 0$, we show that the following two statements hold:
\begin{enumerate}[1)]
    \item $\limsup^* p_n \leq \limsup p_n + \e$
    \item $\limsup^* p_n + \e \geq \limsup p_n$
\end{enumerate}
To show 1), for each $N$, we let $n_N$ be an index such that $p_{n_N}$ satisfies:
\begin{align*}
    p_{n_N} \geq \sup\set{p_n: n \geq N} - \frac{\e}{N}
\end{align*}
And then we claim that $\linf p_{n_N} = \limsup p_n$ (Exercise). There is one slight technical issue in that $\set{p_{n_N}}$ may not be a subsequence of $\set{p_n}$, in particular we don't know that $n_{N_1} < n_{N_2} < n_{N_3} < \ldots$ just by the above construction (but in order for this to be a valid subsequence, we need this to be the case). Fortunately, this is a fixable issue. We do know that $n_{N_1} \geq 1$, $n_{N_2} \geq 2$, $n_{N_3} \geq 3$ by construction, so if it turns out to be the case that $n_{N_1} < n_{N_2}$ doesn't hold, we can skip ahead into the sequence until we find the first $j$ for which the equality holds. Concretely, if $n_{N_1} = 1000$ (as an example), then if we skip ahead to $n_{N_{1001}}$, it is guaranteed that $n_{N_1} < n_{N_{1001}}$ and we can from there construct a valid sequence. The sketch for 2) is left as an exercise. 

\begin{nexample}{}{}
    \begin{enumerate}
        \item Consider $s_n = (-1)^n\left(1 + \frac{1}{n^2}\right)$. Then, we have that $1 \leq \sup_{m \geq n} s_n \leq 1 + \frac{1}{1^2} = 2$, so $\limsup_{n \rightarrow \infty} s_n = 1$. Similarly, $\limsup_{n \rightarrow \infty} s_n = -1$. Note that this sequence has no limit (it oscillates indefinitely and does not converge) but these quantities are well defined. We notice that the limsup is greater than the liminf in this case, and indeed it is true in general that $\liminf_{n \rightarrow \infty} s_n \leq \limsup_{n \rightarrow \infty} s_n$. 
        \item If $\set{s_n}$ is not bounded above, then $\sup_{m \geq n} s_n = \infty$ for all $n$ and we write $\limsup_{n \rightarrow} s_n = \infty$. Similarly, if $\set{s_n}$ is not bounded below, then $\inf_{m \geq n} s_n = -\infty$ for all $n$ and we write $\limsup_{n \rightarrow \infty} s_n = -\infty$.
    \end{enumerate}
\end{nexample}

\noindent One difficulty with discussing the convergence of a sequence is that the definition is difficult to apply; we need to know what the sequence converges to. The notion of a Cauchy sequence then begins helpful (as Cauchy and convergent are equivalent in complete metric spaces). In addition, it is helpful to consider the limsup and liminf, as we can bound the limit above and below with these quantities respectively. In particular, the limit of the supremum of the tail of the sequence equals the limit of the infinimum of the tail of the sequence equals the limit of the sequence if the sequence is convergent.

\setcounter{rudin}{17}
\begin{theorem}{}{3.18}
    Let $\set{s_n} \subset \RR$. Then, $\linf s_n = L$ if and only if $\limsup_{n \rightarrow \infty} s_n = \liminf_{n \rightarrow \infty} = L$.
\end{theorem}
\begin{nproof}
    $\boxed{\implies}$ Let $\e > 0$. Then, there exists $N \in \NN$ such that $s_m \in (L - \e, L + \e)$ for all $m \geq N$> Then, we have that:
    \begin{align*}
        L - \e \leq \inf_{m \geq N} s_m \leq \sup_{m \geq N} \leq L + \e
    \end{align*}
    Taking limits we have:
    \begin{align*}
        L - \e \leq \liminf_{n \rightarrow \infty} \leq \limsup_{n \rightarrow \infty} s_n  \leq L + \e
    \end{align*}
    $\e$ is arbitrary, so we have that $\limsup_{n \rightarrow \infty} s_n = \liminf_{n \rightarrow \infty} = L$ .

    $\boxed{\impliedby}$ We have that:
    \begin{align*}
        \inf_{m \geq n} s_n \leq s_n \leq \sup_{m \geq n} s_m
    \end{align*}
    for all $n \in \NN$. By assumption we have $\limsup_{n \rightarrow \infty} s_n = \liminf_{n \rightarrow \infty} = L$ so by the Generalized Squeeze Lemma we conclude that $\linf s_n = L$. \qed
\end{nproof}
\subsection{Series}

\setcounter{rudin}{20}
\begin{definition}{Infinite Series}{3.21}
    Let $\set{a_n} \subset \CC$. We then form a new sequence of $s_n = \sum_{j=1}^n a_j$ (the sequence of partial sums). Then, if $s_n \rightarrow s$. We say that the series $\sum_{j = 1}^\infty a_j$ converges, and write $\sum_{j = 1}^\infty a_j = s$. If $s_n$ does not converge, we say that $\sum_{j=1}^\infty a_j$ diverges. As a notational point, we will sometimes omit the bounds of summation and write $\sum a_j$ to denote an infinite series, where the meaning is clear from context.
\end{definition}
\noindent Note that series are just a specific subset of sequences. Although the above definition states that $\set{a_n} \subset \CC$, it is in general possible to define series over general vector spaces, with $\set{a_n} \in V$ and $\set{s_n} \in V$. 

Note that this generalization allows us to state an equivalent notion of completeness for vector spaces, namely that $V$ is compelte if and only if for all sequences $\set{a_n} \subset V$, the sum $\sum_{n=0}^\infty \norm{a_n}$ converges to a point in $V$. As before, $\RR, \RR^k$ (both over the field $\RR$), are complete vector spaces. An example of a vector space that is not complete is the set of functions $f: \RR \mapsto \CC$ such that $\int_{-\infty}^\infty \abs{f(x)}^2 dx < \infty$ (where the integral is the familiar Riemann integral from first year calculus, to be defined more rigorously in Chapter 6). For example, the sequence $f_n \subset V$ such that:
\begin{align*}
    f_n = \begin{cases}
        1 & \text{for the first $n$ rationals}
        \\ 0 & \text{elsewhere}
    \end{cases}
\end{align*}
does not converge to a function in $V$. In fact, Lebesgue integration (which is not covered in this course, but will be the primary focus of a course in measure theory) deals with this issue.

We will now restate the Cauchy criterion (Theorem \ref{thm:3.11}) for series.

\begin{theorem}{}{3.22}
    A series $\sum a_j$ converges if and only if for all $\e > 0$, there exists $N \in \NN$ such that $\abs{\sum_{j=m}^n a_j} < \e$ for all $n \geq m \geq N$.
\end{theorem}
\begin{nproof}
    $\sum a_j$ converges if and only if $\set{s_n}$ converges if and only if $\set{s_n}$ is Cauchy. So by definition there exists $N$ such that for $n \geq m - 1 \geq N$:
    \begin{align*}
        \abs{s_n - s_m} = \abs{\sum_{j=1}^n a_j - \sum_{j=1}^{m-1} a_j} = \abs{\sum_{j=m}^n a_j} < \e
    \end{align*}
    which proves the claim. \qed
\end{nproof}

\begin{theorem}{Divergence Test}{3.23}
    If $\sum a_n$ converges, then $\linf a_n = 0$.
\end{theorem}
\begin{nproof}
    Choose $n = m$ in Theorem \ref{thm:3.22}. \qed
\end{nproof}
\noindent Note that this criteria gives us the ability to easily check if a series diverges; if $a_j$ does not converge to $0$, then $\sum a_j$ diverges. However, note that the reverse implication does NOT hold! $\sum \frac{1}{n}$ diverges (as we will show next lecture) even though clearly $\frac{1}{n} \rightarrow 0$. 

\begin{theorem}{}{3.24}
    Let $\set{a_n} \subset \RR$ and $a_n \geq 0$ for all $n$. Then, we have that $\sum a_n$ converges if and only if the sequence of partial sums $\set{s_n}$ is bounded.
\end{theorem}
\begin{nproof}
    If $a_n \geq 0$, then $\set{s_n}$ is monotonically increasing. Then by Theorem \ref{thm:3.14}, $\set{s_n}$ converges if and only if it is bounded. \qed
\end{nproof}

\begin{theorem}{Comparison Test}{3.25}
    \begin{enumerate}
        \item Let $\set{a_n} \subset \CC$ and $\set{c_n} \subset \RR$. Then, if $\abs{a_n} \leq c_n$ for all $n \geq N_0$ for some $N_0 \in \NN$, and $\sum c_n$ converges, then $\sum a_n$ converges. 
        \item Let $\set{a_n} \subset \RR$ and $\set{d_n} \subset \RR$. If $a_n \geq d_n \geq 0$ for all $n \geq N_0$ for some $N_0 \in \NN$ and $\sum d_n$ diverges, then $\sum a_n$ diverges.
    \end{enumerate}
\end{theorem}
\begin{nproof}
    \begin{enumerate}
        \item Let $\e > 0$. Then, there exists $N \in \NN$ such that $\sum_{j = m}^n c_n < \e$ for all $n \geq m \geq N$. Then, take $N \geq N_0$, and we have that:
        \begin{align*}
            \abs{\sum_{j=m}^n a_j} \leq \sum_{j=m}^n \abs{a_j} \leq \sum_{j=m}^n c_j < \e
        \end{align*}
        Where in the first inequality we apply the triangle inequality (Theorem \ref{thm:1.37}). We conclude that $\sum a_j$ converges by the Cauchy criterion (Theorem \ref{thm:3.22}).
        \item The claim follows by considering the contrapositive of (a). \qed
    \end{enumerate}
\end{nproof}

\begin{theorem}{The Geometric Series}{3.26}
    If $0 \leq x < 1$, then $\sum_{j=0}^\infty x^j = \frac{1}{1-x}$. If $x \geq 1$, then $\sum_{j=0}^\infty x^j$ diverges.
\end{theorem}
\begin{nproof}
    Suppose $x = 1$. Then, $x_n = 1$ does not converge to zero, so by Theorem \ref{thm:3.24} $\sum_{j=0}^\infty x^j$ diverges. Suppose then that $x \neq 1$. We have that $s_n = 1 + x + x^2 + \ldots + x^n$. Hence, $xs_n = x + x^2 + x^3 + \ldots + x^n + x^{n+1}$. Hence, $(1-x)s_n = 1 + x^{n+1}$ and therefore a closed form expression for $s_n$ is:
    \begin{align*}
        s_n = \frac{1 + x^{n+1}}{1 - x}
    \end{align*}
    If $x > 1$, we have that $s_n$ diverges (again) by Theorem \ref{thm:3.24}. If $x < 1$, then $x^{n+1} \rightarrow 0$ by \ref{thm:3.20}(e), so $s_n \rightarrow \frac{1}{1-x}$. \qed
\end{nproof}
\noindent Note that by the comparison test, the above result can be generalized to see that $\sum z^j$ for $z \in \CC$ converges for $\abs{z} < 1$ and diverges for $\abs{z} \geq 1$. 

\begin{theorem}{Cauchy Condensation Test}{3.27}
    Suppose $\set{a_n} \subset \RR$ and $a_1 \geq a_2 \geq a_3 \geq \ldots \geq 0$. Then, $\sum a_j$ converges if and only if $\sum_{n=1}^\infty 2^n a_{2^n}$ converges.
\end{theorem}
\begin{nproof}
    $\boxed{\implies}$ For $2^k < n$, using the fact that the sequence is decreasing, we have that:
    \begin{align*}
        a_1 + a_2 + \ldots + a_n &\geq a_1 + a_2 + (a_3 + a_4) + \ldots + (a_{2^{k-1}+1} + \ldots + a_{2^k})
        \\ &\geq \frac{1}{2}a_1 + a_2 + 2a_4 + \ldots + 2^{k-1}a_{2^k}
        \\ &= \frac{1}{2}(a_1 + 2a_2 + 4a_4 + \ldots + 2^ka_{2^k})
    \end{align*}
    Hence by the comparison test (Theorem \ref{thm:3.25}) we have that $\sum 2^n a_{2^n}$ converges if $\sum a_j$ converges.

    $\boxed{\impliedby}$ We show the contrapositive. For $2^k > n$, We have that:
    \begin{align*}
        a_1 + a_2 + \ldots + a_n &\leq a_1 + (a_2 + a_3) + \ldots + (a_{2^k} + \ldots + a_{2^{k+1}-1})
        \\ &\leq a_1 + 2a_2 + 4a_4 + \ldots + 2^ka_{2^k}
    \end{align*}
    Hence by the comparison test (Theorem \ref{thm:3.25}) we have that $\sum 2^n a_{2^n}$ diverges if $\sum a_j$ diverges. \qed
\end{nproof}

\subsection{p-Series and Euler's Number}
We now use the result of Theorem \ref{thm:3.27} to prove a result about a familiar subset of series.

\begin{theorem}{p-Series}{3.28}
    $\sum_{n=1}^\infty \frac{1}{n^p}$ converges if $p > 1$ and diverges if $p \leq 1$. 
\end{theorem}
\begin{nproof}
    If $p \leq 0$, then $n^p$ does not converge to $0$ and hence $\sum \frac{1}{n^p}$ diverges by Theorem \ref{thm:3.23}. If $p > 0$, then $\frac{1}{n^p}$ is a monotonically decreasing sequence of positive terms. Hence, we can apply the result of Theorem \ref{thm:3.27}. $\sum \frac{1}{n^p}$ converges if and only if $\sum_k 2^k \frac{1}{2^{kp}} = \sum_k \left(\frac{1}{2^{p-1}}\right)^k$ converges. By Theorem \ref{thm:3.26}, this last expression is convergent if and only if $0 < \frac{1}{2^{p-1}} < 1$, i.e. if $p > 1$, proving the claim. \qed
\end{nproof}
\noindent From the above result, we have that the $p$-series converges if $p > 1$ and diverges otherwise. Is there something ``in between'' these two regions?

\begin{theorem}{}{3.29}
    $\sum_{n=2}^\infty \frac{1}{n(\log n)^p}$ converges if $p > 1$ and diverges if $p \leq 1$.
\end{theorem}
\begin{nproof}
    $\log n$ is monotonically increasing for $p > 0$. So, $\frac{1}{n(\log n)^p}$ is monotonically decreasing. Hence, $\sum \frac{1}{n(\log n)^p}$ converges if and only if $\sum_k 2^k \frac{1}{2^k}\frac{1}{(\log 2^k)^p} = \sum_k \frac{1}{k^p}$ converges, and hence the claim follows from Theorem \ref{thm:3.28}. \qed
\end{nproof}

\begin{definition}{Euler's Number}{3.30}
    $e = \sum_{n=0}^\infty \frac{1}{n!}$, where $0! = 1$ and $n! = n\cdot (n-1)! = n \cdot (n-1) \cdot \ldots \cdot 2 \cdot 1$ for $n \geq 1$. 
\end{definition}
\noindent We should check that this expression is well defined (namely, that the series converges). We first observe that $\frac{1}{n!} \leq \frac{1}{2^{n-1}}$ for $n \geq 1$. Therefore, we have that:
\begin{align*}
    s_n = \sum_{j=0}^n \frac{1}{j!} \leq 1 + \sum_{j=1}^n \frac{1}{2^{j-1}} \leq 1 + \sum_{j=0}^\infty \frac{1}{2^j} = 1 + \frac{1}{1-\frac{1}{2}} = 3
\end{align*}
Where we use Theorem \ref{thm:3.26} in the second last equality. Hence, we conclude that $\sum_{j=0}^n \frac{1}{j!}$ converges by comparison, and moreover, that $0 < e < 3$. 

It will also be of interest to investigate the rate of convergence of this series. To this end, we observe:

\begin{align*}
    0 < e - s_q = \frac{1}{(q+1)!} + \frac{1}{(q+2)!} + \ldots &\leq \frac{1}{(q+1)!}\left(1 + \frac{1}{q+1} + \frac{1}{(q+1)^2} + \ldots \right) 
    \\ &= \frac{1}{(q+1)!}\frac{1}{1-\frac{1}{q+1}} = \frac{1}{(q+1)!}\frac{q+1}{q} = \frac{1}{q!q}
\end{align*}
Hence we have that the error goes to zero extremely quickly! Moreover, we can use this fact to show that $e$ is irrational.

\setcounter{rudin}{31}
\begin{theorem}{Irrationality of e}{3.32}
    $e \notin \QQ$.
\end{theorem}
\begin{nproof}
    Suppose $e = \frac{p}{q}$ for $p, q \in \NN$. Then, by the argument above, we have that $0 < e - s_q < \frac{1}{q!q}$. Hence, we have that $0 < q! q - q!s_q < \frac{1}{q} \leq 1$. But, $q!e = q!\frac{p}{q} = (q-1)!p \in \NN$, and $q!s_q = \sum_{j=0}^q q!\frac{1}{j!} \in \NN$. Hence, $q! e - q! s_q \in \ZZ$, but this is a contradiction as $0 < q! e  - q! s_q < 1$. \qed
\end{nproof}
\noindent There is another familiar definition of $e$ involving a limit that one may recall from first year calculus. These definitions are equivalent, as we will show in the next theorem.

\setcounter{rudin}{30}
\begin{theorem}{}{3.31}
    $e = \linf \left(1 + \frac{1}{n}\right)^n$.
\end{theorem}
\begin{nproof}
    Let $t_n = \left(1 + \frac{1}{n}\right)^n$. By the Binomial theorem, we have that:
    \begin{align*}
        t_n = \sum_{j=0}^n \binom{n}{j}\left(\frac{1}{n}\right)^j = \sum_{j=0}^n \frac{1}{j!}\left(\frac{n}{n}\cdot\frac{(n-1)}{n}\cdots \frac{(n-j+1)}{n}\right) \leq s_n
    \end{align*}
    Where the last inequality follows from the fact tha the term in brackets is less than (or equal to) 1. Hence, we have that $\limsup_{n \rightarrow \infty} t_n \leq \limsup_{n\rightarrow \infty} s_n = \linf s_n = e$. 

    On the other hand, fix $m \in \NN$ and let $n \geq m$. Then, we have that:
    \begin{align*}
        t_n \geq \sum_{j=0}^m \binom{n}{j}\frac{1}{n^j} = \sum_{j=0}^m \frac{1}{j!}\left(\left(1 - \frac{1}{n}\right)\left(1 - \frac{2}{n}\right) \cdots \left(1 - \frac{j-1}{n}\right)\right)
    \end{align*}
    The infimum of the term in the brackets is just $1$, so we therefore have that:
    \begin{align*}
        \inf_{n \geq m} t_n = \sum_{j=0}^m \frac{1}{j!} = s_m
    \end{align*}
    Now, we take $m \rightarrow \infty$ to find that $\liminf_{m \rightarrow \infty} t_m \geq \liminf_{m \rightarrow \infty} s_m = \lim_{m \rightarrow \infty} s_m = e$. 

    Having shown that $\liminf_{n \rightarrow \infty} t_n \geq e \geq \limsup_{n \rightarrow \infty} t_n$, we conclude that $\limsup_{n \rightarrow \infty} = \liminf_{n \rightarrow \infty} = e$ and hence $\linf t_n = e$. \qed
\end{nproof}

\subsection{The Ratio and Root Tests}
\setcounter{rudin}{32}
\begin{theorem}{The Root Test}{3.33}
    Let $\sum a_n$ be a series, and put $\alpha = \limsup_{n \rightarrow \infty} \sqrt[n]{\abs{a_n}}$. Then,
    \begin{enumerate}[(i)]
        \item $\sum a_n$ converges if $\alpha < 1$.
        \item $\sum a_n$ diverges if $\alpha > 1$. 
        \item If $\alpha = 1$, the test is inconclusive.
    \end{enumerate}
\end{theorem}

\begin{nproof}
    \begin{enumerate}[(i)]
        \item Suppose $\limsup_{n \rightarrow \infty} \sqrt[n]{\abs{a_n}} = \alpha < 1$. Take $\beta$ such that $\alpha < \beta < 1$ and $N \in \NN$ such that $\sqrt[n]{\abs{a_n}} < \beta$ for all $n \geq N$. Hence, for $n \geq N$, $\abs{a_n} < \beta^n$, and $\beta < 1$. The result follows by using the comparison Test with the geometric series.
        \item Suppose $\limsup_{n \rightarrow \infty} \sqrt[n]{\abs{a_n}} = \alpha > 1$. Then, there exists a subsequence such that $\sqrt[n_j]{\abs{a_{n_j}}} \rightarrow \alpha$. Therefore, there exists $N$ such that for $j \geq N$, $\sqrt[n_j]{\abs{a_{n_j}}} > 1$, that is to say, $\sqrt[n_j]{\abs{a_{n_j}}} > 1$ for infinitely many terms. Hence, $\sqrt[n]{\abs{a_{n}}}$ does not converge to zero, and hence the series does not converge by the divergence test.
        \item Consider $\sum \frac{1}{n}$ and $\sum \frac{1}{n^2}$. $\alpha = 1$ for both sums, but by Theorem \ref{thm:3.28} the former diverges and the latter converges. \qed
    \end{enumerate}
\end{nproof}

\begin{theorem}{The Ratio Test}{3.34}
    Let $\sum a_n$ be a series such that $a_n \neq 0$ for all $n$. Then,
    \begin{enumerate}[(i)]
        \item $\sum a_n$ converges if $\limsup_{n \rightarrow \infty} \abs{\frac{a_{n+1}}{a_n}} < 1$. 
        \item Diverges if there exists $N_0$ such that $\abs{\frac{a_{n+1}}{a_n}} \geq 1$ for all $n \geq N_0$. 
    \end{enumerate}
\end{theorem}
\noindent In other cases, the test is inconclusive.
\begin{nproof}
    \begin{enumerate}[(i)]
        \item By assumption, there exists $\beta < 1$ such that for some $N$, $\abs{\frac{a_{n+1}}{a_n}} < \beta$ for all $n \geq N$. We then have that $\abs{a_{N+1}} < \beta \abs{a_N}$, that $\abs{a_{N+1}} < \beta \abs{a_{N+1}} < \beta^2 \abs{a_N}$ and inductively we obtain that $\abs{a_{N+p}} < \beta^p \abs{a_N}$. In other words, we have that for $n \geq N$, $\abs{a_n} < \abs{a_N}\beta^{-N}\beta^{n}$. Since $\sum \beta^n$ converges (convergent geometric series), $\sum a_n$ converges by the comparison test.
        \item For $n \geq N_0$, we have that $\abs{a_n} \leq \abs{a_{N+1}}$. Hence, $a_n$ does not converge to 0, and the claim follows by the divergence test. \qed
    \end{enumerate}
\end{nproof}
\noindent As a remark, the the ratio test is less powerful than the root test. For any series for which the ratio test is conclusive, the root test is also conclusive. But the converse is not true. However, the ratio test is easier to apply in practice. We also note that the above ratio test implies the (perhaps more familiar) version from first year calculus:
\begin{ncorollary}{}{}
    Let $\sum a_n$ be a series such that $a_n \neq 0$ for all $n$. Then:
    \begin{enumerate}[(i)]
        \item $\sum a_n$ converges if $\linf \abs{\frac{a_{n+1}}{a_n}} < 1$.
        \item $\sum a_n$ diverges if $\linf \abs{\frac{a_{n+1}}{a_n}} > 1$.
        \item $\sum a_n$ diverges if $\liminf \abs{\frac{a_{n+1}}{a_n}} > 1$.
    \end{enumerate}
\end{ncorollary}

\begin{example}{}{3.35}
    Consider the series:
    \begin{align*}
        \frac{1}{2} + 1 + \frac{1}{8} + \frac{1}{4} + \frac{1}{32} + \frac{1}{16} + \ldots
    \end{align*}
    We then have that the ratio $\frac{a_{n+1}}{a_n}$ is the sequence $2, \frac{1}{8}, 2, \frac{1}{8}, \ldots$. Therefore, $\limsup_{n \rightarrow \infty} \abs{\frac{a_{n+1}}{a_n}} = 2 > 1$ and $\liminf_{n \rightarrow \infty} \abs{\frac{a_{n+1}}{a_n}} = \frac{1}{8} < 1$ and the ratio test is inconclusive/tells us nothing. We then consider the root test. For $n \geq 3$, we have that:
    \begin{align*}
        a_n = \begin{cases}
            \left(\frac{1}{4}\right)^k = \left(\frac{1}{4}\right)^{\frac{n-1}{2}} = 2\left(\frac{1}{4}\right)^{\frac{n}{2}} & n = 2k + 1
            \\ \frac{1}{2}\left(\frac{1}{4}\right)^{k} = \frac{1}{2}\left(\frac{1}{4}\right)^{\frac{n}{2}} & n = 2k
        \end{cases}
    \end{align*}
    Since $\linf \sqrt[n]{p} = 1$ for $p > 0$ (Theorem \ref{thm:3.20}(b)), we have that:
    \begin{align*}
        \linf \sqrt[n]{a_n} = \linf \sqrt[n]{c\left(\frac{1}{4}\right)^{\frac{n}{2}}} = \linf \sqrt[n]{c} \frac{1}{2} = \frac{1}{2} < 1
    \end{align*}
    where the above limit holds for either $c = \frac{1}{2}$ or $c = 2$. Hence, we conclude that the series converges by the root test. This example demonstrates how the root test is ``sharper'' than the ratio test (though harder to apply).
\end{example}

\subsection{Power Series}

\setcounter{rudin}{37}
\begin{definition}{Power Series}{3.38}
    For $z \in \CC$ and a sequence $\set{c_n}$, $\sum_{n=0}^\infty c_n z^n$ is called a \textbf{power series}.
\end{definition}

\begin{theorem}{Radius of Convergence}{3.39}
    Let $R = \frac{1}{\limsup_{n \rightarrow \infty}\sqrt[n]{\abs{c_n}}}$, with the convention $R = \infty$ if $\limsup_{n \rightarrow \infty}\sqrt[n]{\abs{c_n}} = 0$ and $R = 0$ if $\limsup_{n \rightarrow \infty}\sqrt[n]{\abs{c_n}} = \infty$. Then, $\sum_{n=0}^\infty c_n z^n$ converges if $\abs{z} < R$ and diverges if $\abs{z} > R$. $R$ is called the \textbf{radius of convergence} of $\sum c_n z_n$. We note that on the circle $\abs{z} = R$, the behavior is varied; the series can be divergent or convergent, and it can also depend on the particular choice of $z$ on the circle.
\end{theorem}

\begin{nproof}
    We have that $\limsup_{n \rightarrow \infty} \sqrt[n]{c_n z^n} = \limsup_{n \rightarrow \infty} \sqrt[n]{c_n} \abs{z} = \frac{\abs{z}}{R}$. Therefore, by the root test (Theorem \ref{thm:3.33}) the series converges if $\abs{z} < R$ and diverges if $\abs{z} > R$ (and nothing can be said if $\abs{z} = R$). \qed
\end{nproof}
\noindent Note that we can use the ratio test to determine $R$ as well, as we will see in the next few examples.

\begin{example}{}{3.40}
    \begin{enumerate}
        \item $\sum n! z^n$. By the ratio test, we have that $\linf \abs{\frac{(n+1)!z^{n+1}}{n!z^n}} = \linf (n+1)\abs{z} = \infty$ for all $z \neq 0$. Hence the diverges for all $z \in \CC \neq \set{0}$, and we conclude that $R = 0$. 
        \item $\sum \frac{z^n}{n^n}$. By Theorem \ref{thm:3.39}, we have that:
        \begin{align*}
            R = \frac{1}{\limsup_{n \rightarrow \infty}\sqrt[n]{\frac{1}{n^n}}} = \frac{1}{\limsup_{n \rightarrow \infty} \frac{1}{n}} = \frac{1}{\linf \frac{1}{n}} = \infty
        \end{align*}
        \item $\sum \frac{z^n}{n!}$ also has $R = \infty$ (as can be checked easily with the ratio test). For $R = 1$, the series is equal to $e$. As we will define later in Chapter 8, this series is equal to $e^z$. 
        \item $\sum \frac{z^n}{n^p}$ with $p > 1$. By Theorem \ref{thm:3.39}, we have that:
        \begin{align*}
            R = \frac{1}{\limsup_{n \rightarrow \infty}\sqrt[n]{\frac{1}{n^p}}} = \frac{1}{\limsup_{n \rightarrow \infty} \left(\frac{1}{\sqrt[n]{n}}\right)^p} = \frac{1}{1^p} = 1
        \end{align*}
        where for the second last equality we apply Theorem \ref{thm:3.20}(c). Note that this series converges for all $\abs{z} \leq 1$ (although the above calculation does not show convergence on the boundary).
    \end{enumerate}
\end{example}
\noindent Before moving on, let us consider some further examples. Suppose $a_n = 1$ for all $n$. Then, our power series if just $\sum_{n=0}^\infty z^n$, which is just the Geometric series. By Theorem \ref{thm:3.26}, we have that the series converges if $\abs{z} < 1$ to $\frac{1}{1-z}$.
\begin{figure}[htbp]
    \centering
    \begin{tikzpicture}
        \draw[latex-latex,very thick] (-2,0)--(2,0);
        \draw[latex-latex,very thick] (0,-2)--(0,2);
        \draw[fill = black] (0, 1) circle (1pt);
        \node[xshift = 0.25cm, yshift = 0.25cm] at (0, 1) {$i$};
        \draw[fill = black] (1, 0) circle (1pt);
        \node[xshift = 0.25cm, yshift = 0.25cm] at (1, 0) {$1$};
        \draw[fill = black] (-1, 0) circle (1pt);
        \node[xshift = -0.25cm, yshift = 0.25cm] at (-1, 0) {$-1$};
        \draw[fill = black] (0, -1) circle (1pt);
        \node[xshift = 0.25cm, yshift = -0.25cm] at (0, -1) {$-i$};
        \draw[dotted, fill = white!60!black, fill opacity=0.3] (0, 0) circle (28.5pt);
        \node[] at (1.6, 1.6) {$\CC$};
    \end{tikzpicture}
    \caption{Visualization of the radius of convergence of the Geometric Series. The series converges in the shaded region, and diverges outside of it. On the boundary, the series diverges at $z = 1$ and converges everywhere else.}
    \label{fig16}
\end{figure}


As another example, consider the series $\sum_{n=1}^\infty \frac{1}{n}z^n$. By the ratio test, we find that $R = 1$, as:
\begin{align*}
    \limsup_{n\rightarrow \infty} \abs{\frac{\frac{1}{n+1}z^{n+1}}{\frac{1}{n}z^n}} = \limsup_{n\rightarrow \infty}\abs{\frac{n}{n+1}z} = \abs{z} \limsup_{n\rightarrow \infty}\abs{1 - \frac{1}{n+1}} = \abs{z}
\end{align*}
So this series converges if $\abs{z} < 1$, diverges if $\abs{z} > 1$. What happens for $\abs{z} = 1$? At $z = 1$, we have that the series is just the standard harmonic series and diverges (by Theorem \ref{thm:3.28}). At $z = -1$, we have an alternating series (a series whose terms are decreasing and tend to zero, and alternate in sign with each term), so we can apply the Alternating series test (below) to conclude that it converges. What about elsewhere on the circle? Let's look at $z = i$. We then have that the terms look like:
\begin{align*}
    1 + \frac{i}{2} - \frac{1}{3} - \frac{i}{4} + \frac{1}{5} + \frac{i}{6} - \ldots
\end{align*}
we then have that the real and imaginary parts of the series are separately alternating series that decrease in magnitude; hence both parts are convergent, and the series as a whole is convergent at $z = i$. The same argument can be applied to conclude convergence of the series at $z = -i$. In fact, this series converges everywhere on the unit circle except at $z = 1$, which is a conclusion that follows from Theorem 3.44 (not covered in lecture, but feel free to refer to Rudin)!

\setcounter{rudin}{42}
\begin{theorem}{The Alternating Series Test}{3.43}
    Let $\set{a_n} \subset \CC$ and suppose that:
    \begin{enumerate}
        \item $\abs{a_1} \geq \abs{a_2} \geq \abs{a_3} \ldots$.
        \item $a_{2m-1} \geq 0, a_{2m} \leq 1$ for $m \in \NN$
        \item $\linf a_n = 0$.
    \end{enumerate}
    Then, $\sum a_n$ converges.
\end{theorem}
\begin{nproof}
    Rudin establishes a partial summation formula (Theorem 3.41) and proves a more general theorem (Theorem 3.42) to prove this claim. However, an alternative proof in the case where $\set{a_n}$ is real is left as homework (HW7). \qed
\end{nproof}

\subsection{Absolute Convergence}
\begin{ndef}{: Absolute Convergence}{}
    $\sum a_n$ is \textbf{absolutely convergent} if $\sum \abs{a_n}$ converges. Note that if $\sum a_n$ is convergenct but $\sum \abs{a_n}$ diverges, then $\sum a_n$ is \textbf{conditionally convergent}. Note that for real series with strictly positive terms, absolute convergence and conditional convergence are equivalent. Also, note that the root and ratio tests test for absolute convergence, and hence do not yield any information for conditional convergence. 
\end{ndef}
\noindent As an example, consider that $\sum \frac{(-1)^n}{n}$ converges, but $\sum \frac{1}{n}$ diverges, so $\sum \frac{(-1)^n}{n}$ is conditionally convergent.

\setcounter{rudin}{44}
\begin{theorem}{}{3.45}
    If $\sum a_n$ converges absolutely, then $\sum a_n$ converges.
\end{theorem}
\begin{nproof}
    We have that:
    \begin{align*}
        \abs{\sum_{j=m}^n a_j} \leq \sum_{j=m}^n\abs{a_j} < \e
    \end{align*}
    For all $n \geq m \geq N$ for some $N$ by the fact that $\sum \abs{a_j}$ converges. Hence, $\sum a_j$ is convergent by the Cauchy Criterion. \qed
\end{nproof}
\noindent For absolutely convergent series, we can freely change the order of the additions without affecting the value of the sum (as we will soon see). However, for series that are not absolutely convergent, this turns out to not be the case!

\setcounter{rudin}{51}
\begin{definition}{Rearrangements}{3.52}
    Given a bijection $K: \NN \rightarrow \NN$, the series:
    \begin{align*}
        \sum_n a_n' = \sum_n a_{K(n)}
    \end{align*}
    is called a rearrangement of $\sum_n a_n$. 
\end{definition}

\setcounter{rudin}{54}
\begin{theorem}{}{3.55}
    If $\sum a_n$ is absolutely convergent, every rearrangement $\sum a_n'$ converges to the same limit.
\end{theorem}
\begin{nproof}
    Let $\set{s_n'}$ beth sequence of partial sums of the rearrangement $\sum a_n'$. Let $\e > 0$. By the absolute convergent of the original series, there exists $N \in \NN$ such that for all $n \geq m \geq N$, $\sum_{j=m}^n \abs{a_j} < \e$. Then, pick $p$ such that $\set{1, 2, \ldots N} \subset \set{K(1), K(2), K(3), \ldots K(p)}$. Then, the summands $a_1, a_2, \ldots a_N$ cancel out in $s_n - s_n'$ for $n \geq p$, leaving only terms $a_{K(j)}$ past $a_N$. Hence, $\abs{s_n - s_n'} < \e$ for $n \geq p \geq N$, and we conclude that $\sum a_n'$ converges to the same limit as $\sum a_n$. \qed
\end{nproof}

\setcounter{rudin}{53}
\begin{theorem}{Riemann Rearrangment Theorem}{3.54}
    If $\sum a_n$ is a conditionally convergent series of real numbers, and $- \infty \leq \alpha \leq \beta \leq \infty$, then there is a rearrangement $\sum a_n'$ such that $\liminf_{n \rightarrow \infty} s_n' = \alpha$ and $\limsup_{n \rightarrow \infty} a_n' = \beta$. Taking $\alpha = \beta$, we have that for any real number, there exists a rearrangement that converges to it. 
\end{theorem}
\begin{nproof}
    Not covered in lecture, see Rudin. \qed
\end{nproof}
\noindent For example, given $\sum (-1)^n a_n$ with $a_n \geq 0$ and $\linf a_n = 0$, we can rearrange this series to converge to any point in $\RR$ that we like (for example, $\pi$). The idea is to select positive terms from the series until we overshoot $\pi$, then choose a sequnece of alternating negative/positive terms of decreasing magnitude until the $\e$ distance from $\pi$ decreases to zero.

\subsection{Addition and Multiplication of Series}
\setcounter{rudin}{46}
\begin{theorem}{Series Addition}{3.47}
    Let $\sum a_n = A$ and $\sum b_n = B$. Then, $\sum(a_n + b_n) = A + B$ and $\sum c a_n = cA$ for any fixed $c \in \CC$.
\end{theorem}
\begin{nproof}
    Let $A_n = \sum_{j=0}^n a_j$ and $B_n = \sum_{j=0}^n b_j$. Then, $A_n + B_n = \sum_{j=0}^n a_j + b_j$ and since $\linf A_n = A$ and $\linf B_n = B$, it follows that:
    \begin{align*}
        \linf(A_n + B_n) = A + B.
    \end{align*}
    For the second assertion, we have that $\linf c A_n = c \linf A_n = cA$. \qed
\end{nproof}

\begin{definition}{Series Multiplication}{3.48}
    Let $\sum a_n$ and $\sum b_n$ be two series. Then, the \textbf{product} of of the two series is the series $\sum c_n$ where:
    \begin{align*}
        c_n = \sum_{j=0}^n a_jb_{n-j} = \sum_{j=0}^n a_{n-j}b_j
    \end{align*}
\end{definition}
\noindent Any student who has studied Fourier Series prior to this course will notice the similarity of the above definition to the convolution of two functions.

\setcounter{rudin}{49}
\begin{theorem}{}{3.50}
    Suppose that $\sum a_n$ converges absolutely and $\sum b_n$ converges. Let $\sum a_n = A$ and $\sum b_n = B$. Then, $\sum c_n$ converges and $\sum c_n = AB$.
\end{theorem}
\begin{nproof}
    Let $A_n = \sum_{j=0}^n a_j$ and $B_n = \sum_{j=0}^n b_j$. Let $\beta_n = B_n - B$ (note that $\beta_n \rightarrow 0$). Now, we have that:
    \begin{align*}
        C_n  = \sum_{k=0}^n c_k = \sum_{k=0}^n \sum_{j=0}^k a_j b_{k-j} = \sum_{j=0}^n \sum_{k=j}^n a_j a_{k-j} = \sum_{j=0}^n a_j \sum_{k=j}^n b_{k-j} = \sum_{j=0}^n a_j B_{n-j} = \sum_{j=0}^n a_j(B + \beta_{n-j})
    \end{align*}
    From here, we split the sum and then we have that:
    \begin{align*}
        C_n = \sum_{j=0}^n a_jB + \sum_{j=0}^n a_j\beta_{n-j}
    \end{align*}
    Defining $\gamma_n = \sum_{j=0}^n a_j\beta_{n-j}$ and taking the limit of $n \rightarrow \infty$, we have:
    \begin{align*}
        \linf C_n = C = \linf \left(\sum_{j=0}^n a_jB + \gamma_n\right) = \linf \sum_{j=0}^n a_jB + \linf \gamma_n = AB + \linf \gamma_n
    \end{align*}
    So the claim is proven if $\linf \gamma_n = 0$. Let $\alpha = \sum \abs{a_n} < \infty$ (by assumption of absolute convergence). Let $\e > 0$. Then, tere exists $N \in \NN$ such that $\abs{\beta_j} < \frac{\e}{\alpha}$ for all $j \geq N$ (as $\beta_n \rightarrow 0$). Hence, 
    \begin{align*}
        \abs{\gamma_n} \leq \abs{\sum_{j=0}^n a_{n-j}\beta_j} \leq \abs{\sum_{j=0}^N a_{n-j}\beta_j} + \abs{\sum_{j=N+1}^n a_{n-j}\beta_j} < \abs{\sum_{j=0}^N a_{n-j}\beta_j} + \sum_{j=N+1}^n\abs{a_{n-j}}\frac{\e}{\alpha} \leq \abs{\sum_{j=0}^N a_{n-j}\beta_j} + \e
    \end{align*}
    Letting $n \rightarrow \infty$ with $N$ fixed, we have the first term goes to $0$ as $a_n \rightarrow 0$ as $n \rightarrow 0$. Hence, We have that:
    \begin{align*}
        \linf \abs{\gamma_n} < \e
    \end{align*}
    And as $\e$ is arbitrary, $\linf \abs{\gamma_n} = 0$ and the claim follows. \qed
\end{nproof}